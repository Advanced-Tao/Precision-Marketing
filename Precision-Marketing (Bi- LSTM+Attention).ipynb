{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "'''\n",
    "********************************************************************************************************\n",
    "Precision-Marketing (Bi- LSTM).py\n",
    "class:market\n",
    "\n",
    "author: Andy\n",
    "date:2020/3/18\n",
    "\n",
    "market类的主要目的是分析新浪微博的博文数据（爬取的标签如“会不会得病”、“身体不舒服”等），分析博文中的担忧情绪。\n",
    "因为有担忧情绪的人更有可能购买健康保险，所以这类人群是健康保险产品的潜在客户。\n",
    "\n",
    "该类主要功能有以下三个：\n",
    "1. 导入停用词库和词向量库(函数名：add_stopwords，read_vectors)\n",
    "2. 构建embedding层（函数名：embedding_matrix） \n",
    "3. 将非结构化的文本数据处理成结构化的形式(函数名： word_cut,frequency，recoding，delete)\n",
    "\n",
    "********************************************************************************************************\n",
    "'''\n",
    "class market():\n",
    "\n",
    "\n",
    "    '''\n",
    "name: read_vectors\n",
    "function（函数的功能）: 载入预训练的词向量库中的前topn个词向量\n",
    "input:词向量库的存放路径(path), 导入的词语数目(topn)\n",
    "output: 词向量的维度数（self.dim，如每个词语提取了300个特征，则dim=300）\n",
    "提取的词语数量（self.max_words，如果提取10000个词向量，则self.max_words = 10000）\n",
    "词语字典(self.word_index, 返回的结果是一个字典。每个词语对应一个整数编码\n",
    "比如{\"的\"：1，\"非常\":2，\"生病\":3})\n",
    "整数字典（self.index_word，每一个整数对应一个词语编码，该词典意义不大，已弃用。）\n",
    "词向量(self.vectors，返回的结果是一个字典。每一个词语对应着dim个特征，如果dim=300，则每一个词都对应着一个1×300的向量，\n",
    "比如{\"的\"：array[0.525421355,0.15235234,......],\"非常\":array[0.3535353111,0.3543636321,......]})\n",
    "'''\n",
    "    def read_vectors(self,path, topn):  \n",
    "        lines_num, dim = 0, 0\n",
    "        vectors = {}\n",
    "        iw = []\n",
    "        wi = {}\n",
    "        with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "            first_line = True\n",
    "            for line in f:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    dim = int(line.rstrip().split()[1]) \n",
    "                    continue\n",
    "                lines_num += 1\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "                iw.append(tokens[0]) # iw储存了所有的tokons[0]，意思是index_word\n",
    "                if topn != 0 and lines_num >= topn:\n",
    "                    break\n",
    "        for i, w in enumerate(iw):\n",
    "            wi[w] = i # wi是iw的反转，意思是word_index,用w来储存字符，用一个integer去给字符编码\n",
    "        self.dim = dim\n",
    "        self.max_words = topn\n",
    "        self.word_index = wi\n",
    "        self.index_word = iw\n",
    "        self.vectors = vectors\n",
    "        print(\"Load %s word vectors.\" % len(vectors))\n",
    "\n",
    "        '''\n",
    "name: add_stopwords\n",
    "function: 导入停用词库\n",
    "input:停用词库的存放路径(path)\n",
    "output: 停用词集合（self.stopwords，返回的结果是一个集合(set)，集合中储存了几百个停用词。\n",
    "采用了改进后四川大学提供的停用词库，该词库中加入了保险中的特有名词，如“健康保险”）\n",
    "'''\n",
    "    def add_stopwords(self,path):\n",
    "        stopwords = set()\n",
    "        with open(path,'r',encoding = 'cp936') as file:\n",
    "            for line in file:\n",
    "                stopwords.add(line.strip())\n",
    "        self.stopwords = stopwords\n",
    "        print(\"Load %s stopwords\" %len(stopwords))    \n",
    "\n",
    "        '''\n",
    "name: add_stopwords\n",
    "function: 构建Embedding矩阵，该矩阵维度数目为 词语数量 × 特征数量（比如10000×300），在神经网络中通过该层，\n",
    "可以将每个词语编码成300个维度的密集向量\n",
    "input: 无\n",
    "output: Embedding矩阵（embedding_matrix）\n",
    "'''\n",
    "    def embedding_matrix(self):\n",
    "        embedding_matrix = np.zeros((self.max_words,self.dim))\n",
    "        for word,i in self.word_index.items():\n",
    "            if i < self.max_words:\n",
    "                embedding_vector = self.vectors.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "\n",
    "    '''\n",
    "name: word_cut\n",
    "function: 将博文分割成几个词语\n",
    "input: 储存博文的文档（documents。该文档是一个不定长的list，长度为博文数量，但宽度未知（因为博文不定长），\n",
    "比如[[\"今天胸口疼痛不舒服，自己会不会得病啊？\"][\"上班压力好大，长期下来积劳成疾怎么办\"]......]\n",
    "output: 按照一定规则，将博文切割成词语后的文档（texts，该文档仍然是以个不定长的list，不同词语之间以逗号分割，\n",
    "比如[[\"今天,胸口,疼痛,不舒服，自己,会不会,得病,啊？\"][\"上班,压力,好大，长期下来,积劳成疾,怎么办\"]......]）\n",
    "'''\n",
    "    def word_cut(self,documents):\n",
    "        stopwords = self.stopwords\n",
    "        texts = []\n",
    "        for line in documents:\n",
    "            words = ' '.join(jieba.cut(line)).split(' ') # 用空格去连接，连接后马上又拆分\n",
    "            text = []\n",
    "            for word in words:\n",
    "                if (word not in stopwords) & (word != '')& (word != '\\u3000')& (word != '\\n')&(word != '\\u200b'):\n",
    "                    text.append(word)\n",
    "            texts.append(text)\n",
    "        self.docLength = len(documents)\n",
    "        return(texts)\n",
    "\n",
    "    '''\n",
    "name: frequency\n",
    "function: 按照词语的出现频次过滤掉某些低频次\n",
    "input: 切割成词语后的文档（texts）,允许的最低出现频率（freq，比如freq=5，意味着删掉在所有词语中出现次数 <= 5的词语）\n",
    "output: 过滤后的文档(texts)\n",
    "'''\n",
    "    def frequency(self,texts,freq):\n",
    "        frequency = defaultdict(int) # value为int\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                frequency[word] += 1\n",
    "        texts = [[word for word in text if frequency[word] > freq] for text in texts]\n",
    "        return(texts)\n",
    "\n",
    "    '''\n",
    "name: recoding\n",
    "function: 将词语编码成整数形式，如果词典（word_index）中没有该词语，则编码为-1\n",
    "input: 过滤后的文档(texts)，词典（word_index）\n",
    "output: 将词语按照整数编码后的文档(texts)\n",
    "'''\n",
    "    def recoding(self,texts,word_index):\n",
    "        for i,sample in enumerate(texts):\n",
    "            for j,word in enumerate(sample):\n",
    "                if word not in word_index:\n",
    "                    sample[j] = -1\n",
    "                else:\n",
    "                    sample[j] = word_index[word]\n",
    "            texts[i] = sample\n",
    "        return(texts)\n",
    "\n",
    "    '''\n",
    "name: delete\n",
    "function: 将文档中编码为-1的记录删去\n",
    "input: 将词语按照整数编码后的文档(docs)\n",
    "output: 删除了所有编码为-1的记录的文档（docs）\n",
    "'''\n",
    "    def delete(self,docs):\n",
    "        for index in range(len(docs)):\n",
    "            for i in range(len(docs[index])-1,-1,-1):\n",
    "                if docs[index][i] == -1:\n",
    "                    docs[index].pop(i)\n",
    "        return docs\n",
    "\n",
    "    '''\n",
    "name: random_pick\n",
    "function: 对不担忧的样本做欠采样，比如不担忧的样本有936个，担忧的样本有300个，则欠采样的结果是从不担忧的936个样本里面随机选取300个\n",
    "input: 需要欠采样的数据框（df），欠采样之后的样本数量（n）\n",
    "output: 欠采样后的数据框(df)\n",
    "'''\n",
    "    def random_pick(self,df,n):\n",
    "        rand = np.arange(0,(len(df)-1),1)\n",
    "        random.shuffle(rand)\n",
    "        rand = list(rand[:n])\n",
    "        df = df.loc[rand,]\n",
    "        return(df)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "因为预训练的词向量库中已预含词典，所以该函数被废弃\n",
    "    def dictionary(self,docs):\n",
    "        token_index ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_index:\n",
    "                    token_index[word] = len(token_index) + 1\n",
    "        return(token_index)\n",
    "'''\n",
    "\n",
    "    '''\n",
    "因为预训练的词向量库中已预含词典，所以该函数被废弃\n",
    "    def count(self,docs):\n",
    "        token_length ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_length:\n",
    "                    token_length[word] = 1\n",
    "                else:\n",
    "                    token_length[word] += 1\n",
    "        return(token_length)\n",
    "'''   \n",
    "\n",
    "    '''\n",
    "因为暂停了文本聚类项目，所以该函数被废弃\n",
    "    def regroup(self,texts):\n",
    "        new_texts = []\n",
    "        for i,sentence in enumerate(texts):\n",
    "            new_texts.append(\" \".join(sentence))\n",
    "        return(new_texts)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "K.clear_session()\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self,attention_size = None,**kwargs):\n",
    "        self.attention_size = attention_size\n",
    "        super(AttentionLayer,self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['attention_size'] = self.attention_size\n",
    "        return config\n",
    "    def build(self,input_shape):\n",
    "        assert len(input_shape) == 3,\"Attention层的input_shape长度不等于3\"\n",
    "        \n",
    "        self.time_steps = input_shape[1]\n",
    "        hidden_size = input_shape[2]\n",
    "        # assert self.attention_size is not None, \"请输入正确的Attention_size\"\n",
    "        if self.attention_size is None:\n",
    "            self.attention_size = hidden_size\n",
    "        self.W = self.add_weight(name = 'att_weight',\n",
    "                                 shape = (hidden_size,self.attention_size),\n",
    "                                initializer = 'uniform',trainable = True)\n",
    "        self.b = self.add_weight(name = 'att_bias',shape = (self.attention_size,),\n",
    "                                initializer = 'uniform',trainable = True)\n",
    "        super().build(input_shape)\n",
    "    def call(self,inputs):\n",
    "        M = K.tanh(inputs)\n",
    "        M = K.dot(M,self.W) + self.b\n",
    "        score = K.softmax(M,axis = 1)\n",
    "        r = K.sum(score*inputs,axis = 1)\n",
    "        outputs = K.tanh(r)\n",
    "        return outputs\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return input_shape[0],input_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2316 stopwords\n",
      "Load 20000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 导入停用词库和词向量库\n",
    "# 词向量库由北师大提供(github地址\"https://github.com/Embedding/Chinese-Word-Vectors\")，\n",
    "# 停用词库由四川大学提供(github地址：\"https://github.com/fighting41love/funNLP/tree/master/data/%E5%81%9C%E7%94%A8%E8%AF%8D\")\n",
    "\n",
    "process = market()\n",
    "process.add_stopwords(\"D:/Users/PYTHON/Precision-Marketing/stopwords.txt\")\n",
    "process.read_vectors(\"D:/NLP/sgns.target.word-word.dynwin5.thr10.neg5.dim300.txt\",20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建词矩阵（由若干个词向量堆叠而成，如果导入了10000个词，每个词有300个特征，则矩阵维度为10000×300），\n",
    "# 该步骤可以为下文嵌入keras的embedding层做铺垫\n",
    "embedding_matrix = process.embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共读取了11个sheet\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，数据来源于微博，已经人工标注。数据可以从github上下载\n",
    "# （github地址：https://github.com/Advanced-Tao/Precision-Marketing/master/关键词标签.xlsx）\n",
    "os.chdir(\"D:/Users/PYTHON/Precision-Marketing\")\n",
    "df = pd.DataFrame()\n",
    "num = 0\n",
    "for i in range(11):\n",
    "    df_temp = pd.read_excel(\"关键词标签.xlsx\",sheet_name = i)\n",
    "    df = df.append(df_temp)\n",
    "    num += 1\n",
    "print(\"一共读取了{}个sheet\".format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对无担忧情绪（无买保险欲望）的标签(df_non_worry)做欠采样\n",
    "\n",
    "df = df.loc[pd.notna(df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]),]\n",
    "df_worry = df[df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == 1]\n",
    "df_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = df[df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == -1]\n",
    "df_non_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = process.random_pick(df_non_worry,min(len(df_worry),len(df_non_worry)))\n",
    "df_worry = df_worry[[\"博文\",\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "df_non_worry = df_non_worry[[\"博文\",\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "df_worry = df_worry.dropna()\n",
    "df_non_worry = df_non_worry.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>博文</th>\n",
       "      <th>是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\n                    很多人问我怎嚒下定决心减肥的！因为我怕死，怕以后...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>\\n                    我身边有三个有心理疾病在吃药调节的人所以一般听到...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>\\n                    总是失眠也太难受了天天三四点还睡不着早上七点多爬...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>\\n                    这段时间每天睡得晚，起的晚，可恨没有好好吃早餐。...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>\\n                    突然想起一些事 发现你以为的为别人好其实并不好有...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    博文  \\\n",
       "14   \\n                    很多人问我怎嚒下定决心减肥的！因为我怕死，怕以后...   \n",
       "488  \\n                    我身边有三个有心理疾病在吃药调节的人所以一般听到...   \n",
       "77   \\n                    总是失眠也太难受了天天三四点还睡不着早上七点多爬...   \n",
       "53   \\n                    这段时间每天睡得晚，起的晚，可恨没有好好吃早餐。...   \n",
       "589  \\n                    突然想起一些事 发现你以为的为别人好其实并不好有...   \n",
       "\n",
       "     是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）  \n",
       "14                                             1  \n",
       "488                                           -1  \n",
       "77                                             1  \n",
       "53                                             1  \n",
       "589                                           -1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并欠采样后的无担忧标签和有担忧标签，得到类别数目平衡的数据框df_use\n",
    "\n",
    "df_use = pd.concat([df_worry,df_non_worry])\n",
    "df_use.reset_index(drop = True,inplace = True)\n",
    "df_use = df_use.reindex(np.random.permutation(df_use.index))\n",
    "df_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.617 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(664, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自变量预处理\n",
    "\n",
    "x_train = process.word_cut(df_use[\"博文\"])\n",
    "x_train = process.frequency(x_train,5)\n",
    "x_train = process.recoding(x_train,process.word_index)\n",
    "x_train = process.delete(x_train)\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras import preprocessing\n",
    "\n",
    "max_len = 50\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen = max_len) # 将博文50个字符以后的部分舍弃\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因变量预处理\n",
    "\n",
    "y_train = df_use[[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "y_train[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] = y_train[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"].apply(lambda v: 0 if v == -1 else 1)\n",
    "y_in = len(y_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(y_in)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 40)            51360     \n",
      "_________________________________________________________________\n",
      "attention_layer_1 (Attention (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 6,053,041\n",
      "Trainable params: 53,041\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建神经网络模型\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Bidirectional,Dropout\n",
    "\n",
    "max_features = 20000\n",
    "max_len = 50\n",
    "attention_size = 40\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,300,input_length = max_len)) # 遇到0，就不会反向传播更新权重\n",
    "model.add(Bidirectional(LSTM(20,return_sequences = True,dropout = 0.2)))\n",
    "model.add(AttentionLayer(attention_size = attention_size))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 531 samples, validate on 133 samples\n",
      "Epoch 1/10\n",
      "531/531 [==============================] - 0s 493us/step - loss: 0.5875 - acc: 0.6911 - val_loss: 0.6097 - val_acc: 0.6466\n",
      "Epoch 2/10\n",
      "531/531 [==============================] - 0s 475us/step - loss: 0.5825 - acc: 0.6968 - val_loss: 0.6469 - val_acc: 0.6466\n",
      "Epoch 3/10\n",
      "531/531 [==============================] - 0s 485us/step - loss: 0.5400 - acc: 0.7458 - val_loss: 0.6322 - val_acc: 0.6015\n",
      "Epoch 4/10\n",
      "531/531 [==============================] - 0s 517us/step - loss: 0.5646 - acc: 0.7062 - val_loss: 0.7112 - val_acc: 0.6015\n",
      "Epoch 5/10\n",
      "531/531 [==============================] - 0s 573us/step - loss: 0.5833 - acc: 0.6855 - val_loss: 0.6278 - val_acc: 0.6165\n",
      "Epoch 6/10\n",
      "531/531 [==============================] - 0s 490us/step - loss: 0.5593 - acc: 0.7100 - val_loss: 0.6291 - val_acc: 0.6692\n",
      "Epoch 7/10\n",
      "531/531 [==============================] - 0s 477us/step - loss: 0.5540 - acc: 0.7288 - val_loss: 0.6220 - val_acc: 0.6617\n",
      "Epoch 8/10\n",
      "531/531 [==============================] - 0s 533us/step - loss: 0.5382 - acc: 0.7250 - val_loss: 0.6126 - val_acc: 0.6692\n",
      "Epoch 9/10\n",
      "531/531 [==============================] - 0s 531us/step - loss: 0.5211 - acc: 0.7495 - val_loss: 0.6077 - val_acc: 0.6842\n",
      "Epoch 10/10\n",
      "531/531 [==============================] - 0s 547us/step - loss: 0.5317 - acc: 0.7363 - val_loss: 0.6199 - val_acc: 0.6767\n"
     ]
    }
   ],
   "source": [
    "# 训练神经网络模型\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size =128, # batch_size越大越好，但是太大会影响计算效率\n",
    "                    validation_split= 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
