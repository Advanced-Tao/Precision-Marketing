{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVD分解**\n",
    "\n",
    "当数据已经中心化时，SVD分解等价于主成分分析，它主要起到一个降维的作用。\n",
    "\n",
    "设X是秩为r的n×m矩阵，那么一定存在一个分块矩阵为对角矩阵D的n×m矩阵$\\varSigma$，其中D的对角线元素是X的前r个非零奇异值。所谓奇异值，就是矩阵$X^T X$的特征值的平方根。并且存在一个n×n的正交矩阵U和一个m×m的正交矩阵V，使得：\n",
    "\n",
    "$$\n",
    "X = U\\varSigma V^T\n",
    "$$\n",
    "\n",
    "其中V是$X^T X$的特征向量阵，因为$\\varSigma_x = \\frac{1}{n-1}X^T X $,可以证明V也是协方差矩阵$\\varSigma_x$的特征向量阵。\n",
    "\n",
    "如果作变换$Y = XV$,可以证明：\n",
    "\n",
    "$$\n",
    "\\varSigma_y = \\frac{1}{n-1}Y^T Y = \\frac{1}{n-1}\\varSigma^T \\varSigma\n",
    "$$\n",
    "\n",
    "容易证明$\\varSigma^T \\varSigma/(n-1)$ 也正好是PCA中的特征根对角矩阵D,$D = diag(\\lambda_1,\\lambda_2,...,\\lambda_m)$\n",
    "\n",
    "**标记传播算法**\n",
    "\n",
    "标记传播算法的核心思想是，一个未标记数据的标签应该取决于周围的标签。具体实施的过程中，可以认为一个数据点成为某个标签的概率始终受到周围标签的灌输影响，但是已标签数据的标签始终不会改变。\n",
    "\n",
    "设数据集为$f = [f_1 f_2 ... f_n]$，n是未标记数据点数目与已标记数据点数目的和，$f = f_{p×n}$，p是标签的数目(采用one-hot编码)。\n",
    "\n",
    "设权重矩阵$W = [w_{ij}]$, 归一化矩阵$D = diag(d_{ij}),d = \\Sigma_{j}w_{ij}$，$P = D^{-1}W$而\n",
    "$$\n",
    "w_{ij} =  exp(-\\frac{\\Sigma_k (x_{ik}-x_{jk})^2}{2\\sigma^2})\n",
    "$$\n",
    "\n",
    "执行标签传播，直到到达规定迭代次数或概率值变化足够小为止：\n",
    "\n",
    "$$\n",
    "f = Pf\n",
    "$$\n",
    "\n",
    "然后，取同一个样本中收敛概率值最大的那个标签作为样本标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "import sklearn\n",
    "\n",
    "class market():\n",
    "    def word_cut(self,documents,stopwords):\n",
    "        import jieba\n",
    "        texts = []\n",
    "        for line in documents:\n",
    "            words = ' '.join(jieba.cut(line)).split(' ') # 用空格去连接，连接后马上又拆分\n",
    "            text = []\n",
    "            for word in words:\n",
    "                if (word not in stopwords) & (word != '')& (word != '\\u3000')& (word != '\\n')&(word != '\\u200b'):\n",
    "                    text.append(word)\n",
    "            texts.append(text)\n",
    "        self.docLength = len(documents)\n",
    "        return(texts)\n",
    "    def get_docLength(self):\n",
    "        return(self.docLength)\n",
    "    def frequency(self,texts,freq):\n",
    "        from collections import defaultdict\n",
    "        frequency = defaultdict(int) # value为int\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                frequency[word] += 1\n",
    "        texts = [[word for word in text if frequency[word] > freq] for text in texts]\n",
    "        return(texts)\n",
    "    def regroup(self,texts):\n",
    "        new_texts = []\n",
    "        for i,sentence in enumerate(texts):\n",
    "            new_texts.append(\" \".join(sentence))\n",
    "        return(new_texts)\n",
    "    def add_stopwords(self,path):\n",
    "        stopwords = set()\n",
    "        with open(path,'r',encoding = 'cp936') as file:\n",
    "            for line in file:\n",
    "                stopwords.add(line.strip())\n",
    "        return(stopwords)\n",
    "    def navie_knn(self,dataSet, query, k):  \n",
    "        # 计算出某一样本与所有样本的距离，选择最大(应该修改为最小？)的k个样本作为用于knn\n",
    "        numSamples = dataSet.shape[0] # return row(sample) number of dataset\n",
    "\n",
    "        ## step 1: calculate Euclidean distance  \n",
    "        diff = np.tile(query, (numSamples, 1)) - dataSet #tile: 把query这个向量纵向复制，使得结果与dataset具有同样的行数\n",
    "        squaredDiff = diff ** 2  \n",
    "        squaredDist = np.sum(squaredDiff, axis = 1) # sum is performed by row  \n",
    "\n",
    "        ## step 2: sort the distance  \n",
    "        sortedDistIndices = np.argsort(squaredDist)   # numpy.argsort 返回的是数组值从小到大的索引值（注意是索引值，不是绝对值）\n",
    "        if k > len(sortedDistIndices):  \n",
    "            k = len(sortedDistIndices)  \n",
    "\n",
    "        return sortedDistIndices[0:k]\n",
    "    # build a big graph (normalized weight matrix)  \n",
    "    def buildGraph(self,MatX, kernel_type, rbf_sigma = None, knn_num_neighbors = None):  \n",
    "        num_samples = MatX.shape[0]  # return row(sample) number of MatX\n",
    "        affinity_matrix = np.zeros((num_samples, num_samples), np.float32)  \n",
    "        if kernel_type == 'rbf':  \n",
    "            if rbf_sigma == None:  \n",
    "                raise ValueError('You should input a sigma of rbf kernel!')  \n",
    "            for i in range(num_samples):  \n",
    "                row_sum = 0.0  \n",
    "                for j in range(num_samples):  \n",
    "                    diff = MatX[i, :] - MatX[j, :]  \n",
    "                    affinity_matrix[i][j] = np.exp(sum(diff**2) / (-2.0 * rbf_sigma**2))  \n",
    "                    row_sum += affinity_matrix[i][j]  \n",
    "                affinity_matrix[i][:] /= row_sum  \n",
    "        elif kernel_type == 'knn':  \n",
    "            if knn_num_neighbors == None:  \n",
    "                raise ValueError('You should input a k of knn kernel!')  \n",
    "            for i in range(num_samples):  \n",
    "                k_neighbors = self.navie_knn(MatX, MatX[i, :], knn_num_neighbors)  \n",
    "                affinity_matrix[i][k_neighbors] = 1.0 / knn_num_neighbors  # 将节点i与附近的k个节点连接起来，每个边的权重是1/knn_num_neighbors\n",
    "        else:  \n",
    "            raise NameError('Not support kernel type! You can use knn or rbf!')  \n",
    "\n",
    "        return affinity_matrix  \n",
    "    # label propagation  \n",
    "    def labelPropagation(self,Mat_Label, Mat_Unlabel, labels, kernel_type = 'rbf', rbf_sigma = 0.20, \\\n",
    "                        knn_num_neighbors = 10, max_iter = 500, tol = 1e-3):  \n",
    "        # initialize  \n",
    "        num_label_samples = Mat_Label.shape[0]  #已经标记的sample number\n",
    "        num_unlabel_samples = Mat_Unlabel.shape[0]  #未标记的sample number\n",
    "        num_samples = num_label_samples + num_unlabel_samples\n",
    "        labels_list = np.unique(labels)  #有哪些label\n",
    "        num_classes = len(labels_list)  #label的种类数\n",
    "\n",
    "        MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "        clamp_data_label = np.zeros((num_label_samples, num_classes), np.float32)  \n",
    "        for i in range(num_label_samples):  \n",
    "            clamp_data_label[i][labels[i]] = 1.0   #标记出每一个labelled sample的具体label是什么\n",
    "\n",
    "        label_function = np.zeros((num_samples, num_classes), np.float32)  \n",
    "        label_function[0 : num_label_samples] = clamp_data_label  \n",
    "        label_function[num_label_samples : num_samples] = -1  \n",
    "\n",
    "        # graph construction  \n",
    "        affinity_matrix = self.buildGraph(MatX, kernel_type, rbf_sigma, knn_num_neighbors)  \n",
    "\n",
    "        # start to propagation  \n",
    "        iter = 0; pre_label_function = np.zeros((num_samples, num_classes), np.float32)  \n",
    "        changed = np.abs(pre_label_function - label_function).sum()  \n",
    "        while iter < max_iter and changed > tol:  \n",
    "            if iter % 1 == 0:  \n",
    "                print (\"---> Iteration %d/%d, changed: %f\" % (iter, max_iter, changed))\n",
    "            pre_label_function = label_function  \n",
    "            iter += 1  \n",
    "\n",
    "            # propagation  \n",
    "            label_function = np.dot(affinity_matrix, label_function)  \n",
    "\n",
    "            # clamp  \n",
    "            label_function[0 : num_label_samples] = clamp_data_label  \n",
    "\n",
    "            # check converge  \n",
    "            changed = np.abs(pre_label_function - label_function).sum()  \n",
    "\n",
    "        # get terminate label of unlabeled data  \n",
    "        unlabel_data_labels = np.zeros(num_unlabel_samples)  \n",
    "        for i in range(num_unlabel_samples):  \n",
    "            unlabel_data_labels[i] = np.argmax(label_function[i+num_label_samples]) #取出参数中元素最大值所对应的索引 \n",
    "\n",
    "        return unlabel_data_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,2,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Marketing = market()\n",
    "\n",
    "os.chdir(\"D:/Users/PYTHON/Precision-Marketing\")\n",
    "\n",
    "unlabelled_data = pd.read_csv(\"未标记数据_v2.csv\")\n",
    "unlabelled_data[\"标记情况\"] = \"未标记\"\n",
    "\n",
    "labelled_data = pd.read_csv(\"已标记数据_v2.csv\")\n",
    "labelled_data[\"标记情况\"] = \"已标记\"\n",
    "\n",
    "stopwords = Marketing.add_stopwords(\"stopwords.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>博文</th>\n",
       "      <th>标记情况</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>给好多人说了新年快乐说了新年祝福还没有给自己说过。新年快乐希望在2020年你可以学有所成重要...</td>\n",
       "      <td>已标记</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019开始倒数了哦原来小时候觉得很遥远的2020年就是明天了要说2019年我收获了什么呢？...</td>\n",
       "      <td>已标记</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>微信，QQ都是认识的人，没有办法发泄内心的不满，渐渐养成了不发朋友圈的习惯，但是自己变得越来...</td>\n",
       "      <td>已标记</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>现在已经对酒上瘾很严重每天这个点必须喝半杯而且一天喝水喝特别少大概就一瓶不到..我会不会得病...</td>\n",
       "      <td>已标记</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>这段时间每天睡得晚，起的晚，可恨没有好好吃早餐。会不会得病啊中午上一节男老师的尊巴舞，如果体...</td>\n",
       "      <td>已标记</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  博文 标记情况\n",
       "0  给好多人说了新年快乐说了新年祝福还没有给自己说过。新年快乐希望在2020年你可以学有所成重要...  已标记\n",
       "1  2019开始倒数了哦原来小时候觉得很遥远的2020年就是明天了要说2019年我收获了什么呢？...  已标记\n",
       "2  微信，QQ都是认识的人，没有办法发泄内心的不满，渐渐养成了不发朋友圈的习惯，但是自己变得越来...  已标记\n",
       "3  现在已经对酒上瘾很严重每天这个点必须喝半杯而且一天喝水喝特别少大概就一瓶不到..我会不会得病...  已标记\n",
       "4  这段时间每天睡得晚，起的晚，可恨没有好好吃早餐。会不会得病啊中午上一节男老师的尊巴舞，如果体...  已标记"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = labelled_data[[\"博文\",\"标记情况\"]]\n",
    "labelled_data = labelled_data.dropna()\n",
    "\n",
    "unlabelled_data = unlabelled_data[[\"博文\",\"标记情况\"]]\n",
    "unlabelled_data = unlabelled_data.dropna()\n",
    "\n",
    "all_data = pd.concat([labelled_data,unlabelled_data],axis = 0) \n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['好多 说 新年快乐 说 新年 祝福 说 新年快乐 希望 2020 年 学有所成 崭新 年份 改变 决心 下定决心 做 努力 这才 希望 2020 年 身体健康 平平安安 得病 熬夜 吃 垃圾 食品 点 外卖 顾及 身体 受 影响 小孩子 一天天 变老 希望 2020 年 学会 独立 依赖于 事情 做 害怕 胆子 要学 希望 2020 年 开开心心 想 乱七八糟 事情 控制 情绪 喜怒哀乐 牵扯 身上 那可真 太 糟糕 状况 随波逐流 好好 做 做 做 想 做 做好 做 事过 生活 距离 做事 千万 越界 谨记 一点 生活 关系 希望 越来越 不好 斩断 希望 爱 家人 身体健康 平安 开心 加油 好好 努力 2020 年 学有所成 瘦 独立 好运 爆棚 十年代 收起 全文 d',\n",
       " '2019 倒数 小时候 遥远 2020 年 明天 要说 2019 年 收获 仔细 想想 收获 挺 少 2018 年 月 来到 陌生 地方 大学 生活 2019 末尾 两年 环境 特别 敏感 喜欢 坏 情绪 发泄 身上 朋友 袒露 心声 忽略 友情 独立 特别 抑郁 暴躁 细数 种种 丝毫 改变 变本加厉 是从 想 摔 东西 想 打人 想 哭 想 伤心 事会 世界 喜欢 总 想着 讨厌 死 想着 出 一场 事故 治 绝症 快点 死 掉 想过 方式 自杀 晚上 睡不着 很困 想 睡觉 暴躁 意识 时 高中 伴随 高考 放假 一大 二 说长 不长 说 短 短 几年 隐藏 说 不能不要 装 一点 快乐 家人 任性 爱闹 有时候 闷 亲戚 邻居 内敛 不爱 说话 礼貌 老师 同学 朋友 爱 笑 爱 闹 相处 开心果 陌生人 礼貌 善于 交际 相处 好多年 朋友 光 想着 忽略 没有勇气 胆量 倾诉 朋友 袒露 一点 害怕 倾诉 不到 预期 中 安慰 鼓励 害怕 很装 乐观 奇葩 脑子 远离 告诉 其他人 孤立 光 想 害怕 不行 拼命 开心 伤心 难过 努力 安慰 有时候 还会 干 吃力不讨好 事情 埋怨 只会 闷在心里 原因 暴躁 感觉 全世界 做 地方 发泄 消化 掉 真的 羡慕 说 始终 敢 写成 字 文字 中 拼命 呐喊 一点 快乐 快乐 好像 得病 好像 坏掉 活不下去 说 一大 推 废话 2020 年 活着 收起 全文 d']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = Marketing.word_cut(all_data[\"博文\"],stopwords)\n",
    "\n",
    "docs = Marketing.frequency(docs,0)\n",
    "\n",
    "docs = Marketing.regroup(docs)\n",
    "\n",
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档向量化\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(docs)\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据的方差贡献率：42.99 %\n"
     ]
    }
   ],
   "source": [
    "# SVD降维\n",
    "\n",
    "n_pick_topics = 400\n",
    "\n",
    "lsa = TruncatedSVD(n_pick_topics) \n",
    "\n",
    "lsa.fit(tfidf_matrix)\n",
    "print(\"数据的方差贡献率：{:.2f} %\".format(lsa.explained_variance_ratio_.sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_SVD = lsa.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (0,1,2,4,5,6,7,8,9,10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"已标记数据_v2.csv\")\n",
    "labels = labels[\"是否担忧（1=担忧，0=不担忧）\"]\n",
    "labels = labels.dropna()\n",
    "labels = np.array(labels)\n",
    "labels=labels.astype(np.int16)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523\n",
      "2014\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec_labelled = tfidf_matrix_SVD[:len(labels)]\n",
    "tfidf_vec_unlabelled = tfidf_matrix_SVD[len(labels):]\n",
    "print(tfidf_vec_labelled.shape[0])\n",
    "print(tfidf_vec_unlabelled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Iteration 0/500, changed: 300.000000\n",
      "---> Iteration 1/500, changed: 139.430252\n",
      "---> Iteration 2/500, changed: 72.514725\n",
      "---> Iteration 3/500, changed: 38.830555\n",
      "---> Iteration 4/500, changed: 21.354744\n",
      "---> Iteration 5/500, changed: 11.958540\n",
      "---> Iteration 6/500, changed: 6.777044\n",
      "---> Iteration 7/500, changed: 3.871007\n",
      "---> Iteration 8/500, changed: 2.222786\n",
      "---> Iteration 9/500, changed: 1.280938\n",
      "---> Iteration 10/500, changed: 0.740004\n",
      "---> Iteration 11/500, changed: 0.428240\n",
      "---> Iteration 12/500, changed: 0.248128\n",
      "---> Iteration 13/500, changed: 0.143892\n",
      "---> Iteration 14/500, changed: 0.083498\n",
      "---> Iteration 15/500, changed: 0.048474\n",
      "---> Iteration 16/500, changed: 0.028149\n",
      "---> Iteration 17/500, changed: 0.016352\n",
      "---> Iteration 18/500, changed: 0.009501\n",
      "---> Iteration 19/500, changed: 0.005519\n",
      "---> Iteration 20/500, changed: 0.003208\n",
      "---> Iteration 21/500, changed: 0.001861\n",
      "---> Iteration 22/500, changed: 0.001082\n"
     ]
    }
   ],
   "source": [
    "unlabel_data_labels=Marketing.labelPropagation(tfidf_vec_labelled, tfidf_vec_unlabelled, labels, kernel_type = 'rbf', rbf_sigma = 0.20, \\\n",
    "                                     knn_num_neighbors = 25, max_iter = 500, tol = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({\"PREDICTED\" : list(unlabel_data_labels)})\n",
    "acc[\"PREDICTED\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = clf.labels_\n",
    "# LABEL = pd.DataFrame({\"聚类结果\":label})\n",
    "# LABEL.to_excel(\"聚类结果.xlsx\",header = True)\n",
    "\n",
    "# unlabels= unlabelled_data[\"是否担忧（1=担忧，0=不担忧）\"]\n",
    "# unlabels = unlabels.dropna()\n",
    "\n",
    "# acc = pd.DataFrame({\"PREDICTED\" : list(unlabel_data_labels),\"ACTUAL\":unlabels})\n",
    "# acc[\"PREDICTED\"] = acc[\"PREDICTED\"].apply(lambda r:\"担忧\" if r == 1 else \"不担忧\")\n",
    "# acc[\"ACTUAL\"] = acc[\"ACTUAL\"].apply(lambda r:\" 担忧\" if r == 1 else \"不担忧\")\n",
    "# sklearn.metrics.confusion_matrix(acc[\"ACTUAL\"], acc[\"PREDICTED\"], labels=[\"担忧\",\"不担忧\"], sample_weight=None)\n",
    "# acc.to_excel(\"RESULT.xlsx\",header = True)\n",
    "\n",
    "#  acc.to_excel(\"第三次测试.xlsx\",header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
