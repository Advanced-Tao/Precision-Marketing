{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "'''\n",
    "********************************************************************************************************\n",
    "Precision-Marketing (Bi- LSTM).py\n",
    "class:market\n",
    "\n",
    "author: Andy\n",
    "date:2020/3/18\n",
    "\n",
    "market类的主要目的是分析新浪微博的博文数据（爬取的标签如“会不会得病”、“身体不舒服”等），分析博文中的担忧情绪。\n",
    "因为有担忧情绪的人更有可能购买健康保险，所以这类人群是健康保险产品的潜在客户。\n",
    "\n",
    "该类主要功能有以下三个：\n",
    "1. 导入停用词库和词向量库(函数名：add_stopwords，read_vectors)\n",
    "2. 构建embedding层（函数名：embedding_matrix） \n",
    "3. 将非结构化的文本数据处理成结构化的形式(函数名： word_cut,frequency，recoding，delete)\n",
    "\n",
    "********************************************************************************************************\n",
    "'''\n",
    "class market():\n",
    "    '''\n",
    "name: read_vectors\n",
    "function（函数的功能）: 载入预训练的词向量库中的前topn个词向量\n",
    "input:词向量库的存放路径(path), 导入的词语数目(topn)\n",
    "output: 词向量的维度数（self.dim，如每个词语提取了300个特征，则dim=300）\n",
    "提取的词语数量（self.max_words，如果提取10000个词向量，则self.max_words = 10000）\n",
    "词语字典(self.word_index, 返回的结果是一个字典。每个词语对应一个整数编码\n",
    "比如{\"的\"：1，\"非常\":2，\"生病\":3})\n",
    "整数字典（self.index_word，每一个整数对应一个词语编码，该词典意义不大，已弃用。）\n",
    "词向量(self.vectors，返回的结果是一个字典。每一个词语对应着dim个特征，如果dim=300，则每一个词都对应着一个1×300的向量，\n",
    "比如{\"的\"：array[0.525421355,0.15235234,......],\"非常\":array[0.3535353111,0.3543636321,......]})\n",
    "'''\n",
    "\n",
    "\n",
    "    def read_vectors(self,path, topn):  \n",
    "        lines_num, dim = 0, 0\n",
    "        vectors = {}\n",
    "        iw = []\n",
    "        wi = {}\n",
    "        with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "            first_line = True\n",
    "            for line in f:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    dim = int(line.rstrip().split()[1]) \n",
    "                    continue\n",
    "                lines_num += 1\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "                iw.append(tokens[0]) # iw储存了所有的tokons[0]，意思是index_word\n",
    "                if topn != 0 and lines_num >= topn:\n",
    "                    break\n",
    "        for i, w in enumerate(iw):\n",
    "            wi[w] = i # wi是iw的反转，意思是word_index,用w来储存字符，用一个integer去给字符编码\n",
    "        self.dim = dim\n",
    "        self.max_words = topn\n",
    "        self.word_index = wi\n",
    "        self.index_word = iw\n",
    "        self.vectors = vectors\n",
    "        print(\"Load %s word vectors.\" % len(vectors))\n",
    "    '''\n",
    "name: add_stopwords\n",
    "function: 导入停用词库\n",
    "input:停用词库的存放路径(path)\n",
    "output: 停用词集合（self.stopwords，返回的结果是一个集合(set)，集合中储存了几百个停用词。\n",
    "采用了改进后四川大学提供的停用词库，该词库中加入了保险中的特有名词，如“健康保险”）\n",
    "'''\n",
    "    def add_stopwords(self,path):\n",
    "        stopwords = set()\n",
    "        with open(path,'r',encoding = 'cp936') as file:\n",
    "            for line in file:\n",
    "                stopwords.add(line.strip())\n",
    "        self.stopwords = stopwords\n",
    "        print(\"Load %s stopwords\" %len(stopwords))    \n",
    "    '''\n",
    "name: add_stopwords\n",
    "function: 构建Embedding矩阵，该矩阵维度数目为 词语数量 × 特征数量（比如10000×300），在神经网络中通过该层，\n",
    "可以将每个词语编码成300个维度的密集向量\n",
    "input: 无\n",
    "output: Embedding矩阵（embedding_matrix）\n",
    "'''\n",
    "    def embedding_matrix(self):\n",
    "        embedding_matrix = np.zeros((self.max_words,self.dim))\n",
    "        for word,i in self.word_index.items():\n",
    "            if i < self.max_words:\n",
    "                embedding_vector = self.vectors.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    '''\n",
    "name: word_cut\n",
    "function: 将博文分割成几个词语\n",
    "input: 储存博文的文档（documents。该文档是一个不定长的list，长度为博文数量，但宽度未知（因为博文不定长），\n",
    "比如[[\"今天胸口疼痛不舒服，自己会不会得病啊？\"][\"上班压力好大，长期下来积劳成疾怎么办\"]......]\n",
    "output: 按照一定规则，将博文切割成词语后的文档（texts，该文档仍然是以个不定长的list，不同词语之间以逗号分割，\n",
    "比如[[\"今天,胸口,疼痛,不舒服，自己,会不会,得病,啊？\"][\"上班,压力,好大，长期下来,积劳成疾,怎么办\"]......]）\n",
    "'''\n",
    "\n",
    "    def word_cut(self,documents):\n",
    "        stopwords = self.stopwords\n",
    "        texts = []\n",
    "        for line in documents:\n",
    "            words = ' '.join(jieba.cut(line)).split(' ') # 用空格去连接，连接后马上又拆分\n",
    "            text = []\n",
    "            for word in words:\n",
    "                if (word not in stopwords) & (word != '')& (word != '\\u3000')& (word != '\\n')&(word != '\\u200b'):\n",
    "                    text.append(word)\n",
    "            texts.append(text)\n",
    "        self.docLength = len(documents)\n",
    "        return(texts)\n",
    "    '''\n",
    "name: frequency\n",
    "function: 按照词语的出现频次过滤掉某些低频次\n",
    "input: 切割成词语后的文档（texts）,允许的最低出现频率（freq，比如freq=5，意味着删掉在所有词语中出现次数 <= 5的词语）\n",
    "output: 过滤后的文档(texts)\n",
    "'''\n",
    "    def frequency(self,texts,freq):\n",
    "        frequency = defaultdict(int) # value为int\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                frequency[word] += 1\n",
    "        texts = [[word for word in text if frequency[word] > freq] for text in texts]\n",
    "        return(texts)\n",
    "    '''\n",
    "name: recoding\n",
    "function: 将词语编码成整数形式，如果词典（word_index）中没有该词语，则编码为-1\n",
    "input: 过滤后的文档(texts)，词典（word_index）\n",
    "output: 将词语按照整数编码后的文档(texts)\n",
    "'''\n",
    "    def recoding(self,texts,word_index):\n",
    "        for i,sample in enumerate(texts):\n",
    "            for j,word in enumerate(sample):\n",
    "                if word not in word_index:\n",
    "                    sample[j] = -1\n",
    "                else:\n",
    "                    sample[j] = word_index[word]\n",
    "            texts[i] = sample\n",
    "        return(texts)\n",
    "    '''\n",
    "name: delete\n",
    "function: 将文档中编码为-1的记录删去\n",
    "input: 将词语按照整数编码后的文档(docs)\n",
    "output: 删除了所有编码为-1的记录的文档（docs）\n",
    "'''\n",
    "    def delete(self,docs):\n",
    "        for index in range(len(docs)):\n",
    "            for i in range(len(docs[index])-1,-1,-1):\n",
    "                if docs[index][i] == -1:\n",
    "                    docs[index].pop(i)\n",
    "        return docs\n",
    "    \n",
    "    '''\n",
    "name: random_pick\n",
    "function: 对不担忧的样本做欠采样，比如不担忧的样本有936个，担忧的样本有300个，则欠采样的结果是从不担忧的936个样本里面随机选取300个\n",
    "input: 需要欠采样的数据框（df），欠采样之后的样本数量（n）\n",
    "output: 欠采样后的数据框(df)\n",
    "'''\n",
    "    def random_pick(self,df,n):\n",
    "        rand = np.arange(0,(len(df)-1),1)\n",
    "        random.shuffle(rand)\n",
    "        rand = list(rand[:n])\n",
    "        df = df.loc[rand,]\n",
    "        return(df)\n",
    "    '''\n",
    "name: regroup\n",
    "function:将逗号分割的词语以空格连接\n",
    "'''\n",
    "    def regroup(self,texts):\n",
    "        new_texts = []\n",
    "        for i,sentence in enumerate(texts):\n",
    "            new_texts.append(\" \".join(sentence))\n",
    "        return(new_texts)\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "因为预训练的词向量库中已预含词典，所以该函数被废弃\n",
    "    def dictionary(self,docs):\n",
    "        token_index ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_index:\n",
    "                    token_index[word] = len(token_index) + 1\n",
    "        return(token_index)\n",
    "'''\n",
    "\n",
    "\n",
    "    '''\n",
    "因为预训练的词向量库中已预含词典，所以该函数被废弃\n",
    "    def count(self,docs):\n",
    "        token_length ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_length:\n",
    "                    token_length[word] = 1\n",
    "                else:\n",
    "                    token_length[word] += 1\n",
    "        return(token_length)\n",
    "'''   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共读取了11个sheet\n"
     ]
    }
   ],
   "source": [
    "# 导入数据，数据来源于微博，已经人工标注。数据可以从github上下载\n",
    "# （github地址：https://github.com/Advanced-Tao/Precision-Marketing/master/关键词标签.xlsx）\n",
    "os.chdir(\"D:/Users/PYTHON/Precision-Marketing\")\n",
    "df = pd.DataFrame()\n",
    "num = 0\n",
    "for i in range(11):\n",
    "    df_temp = pd.read_excel(\"关键词标签.xlsx\",sheet_name = i)\n",
    "    df = df.append(df_temp)\n",
    "    num += 1\n",
    "print(\"一共读取了{}个sheet\".format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2316 stopwords\n"
     ]
    }
   ],
   "source": [
    "process = market()\n",
    "process.add_stopwords(\"D:/Users/PYTHON/Precision-Marketing/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）</th>\n",
       "      <th>博文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>\\n                    一直在各个公开场合严肃说了要理智告诉粉丝应该怎样...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>\\n                    #分手了怎么挽回前任##失恋后怎么挽回前任# 父...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）  \\\n",
       "0                                           -1   \n",
       "2                                           -1   \n",
       "\n",
       "                                                  博文  \n",
       "0  \\n                    一直在各个公开场合严肃说了要理智告诉粉丝应该怎样...  \n",
       "2  \\n                    #分手了怎么挽回前任##失恋后怎么挽回前任# 父...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[pd.notna(df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]),]\n",
    "df = df[[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\",\"博文\"]]\n",
    "df = df[(df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == -1) | (df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == 1)]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"博文\"]\n",
    "X = process.word_cut(X)\n",
    "X = process.frequency(X,5)\n",
    "X = process.regroup(X)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(X)\n",
    "X = X.toarray()\n",
    "\n",
    "y = np.array(df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,y,random_state=42,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive = MultinomialNB()\n",
    "naive.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 343 points : 110\n"
     ]
    }
   ],
   "source": [
    "y_pred = naive.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
