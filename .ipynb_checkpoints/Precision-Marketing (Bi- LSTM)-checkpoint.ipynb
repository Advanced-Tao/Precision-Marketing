{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class market():\n",
    "    def word_cut(self,documents):\n",
    "        stopwords = self.stopwords\n",
    "        import jieba\n",
    "        texts = []\n",
    "        for line in documents:\n",
    "            words = ' '.join(jieba.cut(line)).split(' ') # 用空格去连接，连接后马上又拆分\n",
    "            text = []\n",
    "            for word in words:\n",
    "                if (word not in stopwords) & (word != '')& (word != '\\u3000')& (word != '\\n')&(word != '\\u200b'):\n",
    "                    text.append(word)\n",
    "            texts.append(text)\n",
    "        self.docLength = len(documents)\n",
    "        return(texts)\n",
    "    def get_docLength(self):\n",
    "        return(self.docLength)\n",
    "    def frequency(self,texts,freq):\n",
    "        from collections import defaultdict\n",
    "        frequency = defaultdict(int) # value为int\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                frequency[word] += 1\n",
    "        texts = [[word for word in text if frequency[word] > freq] for text in texts]\n",
    "        return(texts)\n",
    "    def regroup(self,texts):\n",
    "        new_texts = []\n",
    "        for i,sentence in enumerate(texts):\n",
    "            new_texts.append(\" \".join(sentence))\n",
    "        return(new_texts)\n",
    "    def add_stopwords(self,path):\n",
    "        stopwords = set()\n",
    "        with open(path,'r',encoding = 'cp936') as file:\n",
    "            for line in file:\n",
    "                stopwords.add(line.strip())\n",
    "        self.stopwords = stopwords\n",
    "        print(\"Load %s stopwords\" %len(stopwords))\n",
    "    def dictionary(self,docs):\n",
    "        token_index ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_index:\n",
    "                    token_index[word] = len(token_index) + 1\n",
    "        return(token_index)\n",
    "    def count(self,docs):\n",
    "        token_length ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_length:\n",
    "                    token_length[word] = 1\n",
    "                else:\n",
    "                    token_length[word] += 1\n",
    "        return(token_length)\n",
    "    def recoding(self,docs,token_index):\n",
    "        for i,sample in enumerate(docs):\n",
    "            for j,word in enumerate(sample):\n",
    "                if word not in token_index:\n",
    "                    sample[j] = -1\n",
    "                else:\n",
    "                    sample[j] = token_index[word]\n",
    "            docs[i] = sample\n",
    "        return(docs)\n",
    "    def delete(self,docs):\n",
    "        for index in range(len(docs)):\n",
    "            for i in range(len(docs[index])-1,-1,-1):\n",
    "                if docs[index][i] == -1:\n",
    "                    docs[index].pop(i)\n",
    "        return docs\n",
    "    def random_pick(self,df,n):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        rand = np.arange(0,(len(df)-1),1)\n",
    "        random.shuffle(rand)\n",
    "        rand = list(rand[:n])\n",
    "        df = df.loc[rand,]\n",
    "        return(df)\n",
    "    def read_vectors(self,path, topn):  # read top n word vectors, i.e. top is 10000\n",
    "        lines_num, dim = 0, 0\n",
    "        vectors = {}\n",
    "        iw = []\n",
    "        wi = {}\n",
    "        with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "            first_line = True\n",
    "            for line in f:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    dim = int(line.rstrip().split()[1]) # 删除向量末尾的空格，然后以空格拆分获得向量\n",
    "                    continue\n",
    "                lines_num += 1\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])# 当数据源是ndarray时，asarray不会占用新的内存；当数据源不是ndarray,asarray与array一样\n",
    "                iw.append(tokens[0]) # iw储存了所有的tokons[0]，意思是index_word\n",
    "                if topn != 0 and lines_num >= topn:\n",
    "                    break\n",
    "        for i, w in enumerate(iw):\n",
    "            wi[w] = i # wi是iw的反转，意思是word_index,用w来储存字符，用一个integer去给字符编码\n",
    "        self.dim = dim\n",
    "        self.max_words = topn\n",
    "        self.word_index = wi\n",
    "        self.index_word = iw\n",
    "        self.vectors = vectors\n",
    "        print(\"Load %s word vectors.\" % len(vectors))\n",
    "    def embedding_matrix(self):\n",
    "        embedding_matrix = np.zeros((self.max_words,self.dim))\n",
    "        for word,i in self.word_index.items():\n",
    "            if i < self.max_words:\n",
    "                embedding_vector = self.vectors.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    def navie_knn(self,dataSet, query, k):  \n",
    "        # 计算出某一样本与所有样本的距离，选择最大(应该修改为最小？)的k个样本作为用于knn\n",
    "        numSamples = dataSet.shape[0] # return row(sample) number of dataset\n",
    "\n",
    "        ## step 1: calculate Euclidean distance  \n",
    "        diff = np.tile(query, (numSamples, 1)) - dataSet #tile: 把query这个向量纵向复制，使得结果与dataset具有同样的行数\n",
    "        squaredDiff = diff ** 2  \n",
    "        squaredDist = np.sum(squaredDiff, axis = 1) # sum is performed by row  \n",
    "\n",
    "        ## step 2: sort the distance  \n",
    "        sortedDistIndices = np.argsort(squaredDist)   # numpy.argsort 返回的是数组值从小到大的索引值（注意是索引值，不是绝对值）\n",
    "        if k > len(sortedDistIndices):  \n",
    "            k = len(sortedDistIndices)  \n",
    "\n",
    "        return sortedDistIndices[0:k]\n",
    "    # build a big graph (normalized weight matrix)  \n",
    "    def buildGraph(self,MatX, kernel_type, rbf_sigma = None, knn_num_neighbors = None):  \n",
    "        num_samples = MatX.shape[0]  # return row(sample) number of MatX\n",
    "        affinity_matrix = np.zeros((num_samples, num_samples), np.float32)  \n",
    "        if kernel_type == 'rbf':  \n",
    "            if rbf_sigma == None:  \n",
    "                raise ValueError('You should input a sigma of rbf kernel!')  \n",
    "            for i in range(num_samples):  \n",
    "                row_sum = 0.0  \n",
    "                for j in range(num_samples):  \n",
    "                    diff = MatX[i, :] - MatX[j, :]  \n",
    "                    affinity_matrix[i][j] = np.exp(sum(diff**2) / (-2.0 * rbf_sigma**2))  \n",
    "                    row_sum += affinity_matrix[i][j]  \n",
    "                affinity_matrix[i][:] /= row_sum  \n",
    "        elif kernel_type == 'knn':  \n",
    "            if knn_num_neighbors == None:  \n",
    "                raise ValueError('You should input a k of knn kernel!')  \n",
    "            for i in range(num_samples):  \n",
    "                k_neighbors = self.navie_knn(MatX, MatX[i, :], knn_num_neighbors)  \n",
    "                affinity_matrix[i][k_neighbors] = 1.0 / knn_num_neighbors  # 将节点i与附近的k个节点连接起来，每个边的权重是1/knn_num_neighbors\n",
    "        else:  \n",
    "            raise NameError('Not support kernel type! You can use knn or rbf!')  \n",
    "\n",
    "        return affinity_matrix  \n",
    "    # label propagation  \n",
    "    def labelPropagation(self,Mat_Label, Mat_Unlabel, labels, kernel_type = 'rbf', rbf_sigma = 0.20, \\\n",
    "                        knn_num_neighbors = 10, max_iter = 500, tol = 1e-3):  \n",
    "        # initialize  \n",
    "        num_label_samples = Mat_Label.shape[0]  #已经标记的sample number\n",
    "        num_unlabel_samples = Mat_Unlabel.shape[0]  #未标记的sample number\n",
    "        num_samples = num_label_samples + num_unlabel_samples\n",
    "        labels_list = np.unique(labels)  #有哪些label\n",
    "        num_classes = len(labels_list)  #label的种类数\n",
    "\n",
    "        MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "        clamp_data_label = np.zeros((num_label_samples, num_classes), np.float32)  \n",
    "        for i in range(num_label_samples):  \n",
    "            clamp_data_label[i][labels[i]] = 1.0   #标记出每一个labelled sample的具体label是什么\n",
    "\n",
    "        label_function = np.zeros((num_samples, num_classes), np.float32)  \n",
    "        label_function[0 : num_label_samples] = clamp_data_label  \n",
    "        label_function[num_label_samples : num_samples] = -1  \n",
    "\n",
    "        # graph construction  \n",
    "        affinity_matrix = self.buildGraph(MatX, kernel_type, rbf_sigma, knn_num_neighbors)  \n",
    "\n",
    "        # start to propagation  \n",
    "        iter = 0; pre_label_function = np.zeros((num_samples, num_classes), np.float32)  \n",
    "        changed = np.abs(pre_label_function - label_function).sum()  \n",
    "        while iter < max_iter and changed > tol:  \n",
    "            if iter % 1 == 0:  \n",
    "                print (\"---> Iteration %d/%d, changed: %f\" % (iter, max_iter, changed))\n",
    "            pre_label_function = label_function  \n",
    "            iter += 1  \n",
    "\n",
    "            # propagation  \n",
    "            label_function = np.dot(affinity_matrix, label_function)  \n",
    "\n",
    "            # clamp  \n",
    "            label_function[0 : num_label_samples] = clamp_data_label  \n",
    "\n",
    "            # check converge  \n",
    "            changed = np.abs(pre_label_function - label_function).sum()  \n",
    "\n",
    "        # get terminate label of unlabeled data  \n",
    "        unlabel_data_labels = np.zeros(num_unlabel_samples)  \n",
    "        for i in range(num_unlabel_samples):  \n",
    "            unlabel_data_labels[i] = np.argmax(label_function[i+num_label_samples]) #取出参数中元素最大值所对应的索引 \n",
    "\n",
    "        return unlabel_data_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2316 stopwords\n",
      "Load 10000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "process = market()\n",
    "process.add_stopwords(\"D:/Users/PYTHON/Precision-Marketing/stopwords.txt\")\n",
    "process.read_vectors(\"D:/NLP/sgns.target.word-word.dynwin5.thr10.neg5.dim300.txt\",10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = process.embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共读取了10个sheet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>用户名</th>\n",
       "      <th>博文</th>\n",
       "      <th>关键词</th>\n",
       "      <th>是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）</th>\n",
       "      <th>标签1（担忧对象）</th>\n",
       "      <th>标签2（担忧什么）</th>\n",
       "      <th>标签3（什么保险）</th>\n",
       "      <th>标签4（症状）</th>\n",
       "      <th>转发数</th>\n",
       "      <th>评论数</th>\n",
       "      <th>点赞数</th>\n",
       "      <th>发文时间</th>\n",
       "      <th>来自</th>\n",
       "      <th>页面网址</th>\n",
       "      <th>博文链接</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1杯冰牛奶</td>\n",
       "      <td>\\n                    一直在各个公开场合严肃说了要理智告诉粉丝应该怎样...</td>\n",
       "      <td>父母怎么办</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n                        今天06:16\\n           ...</td>\n",
       "      <td>iPhone 11</td>\n",
       "      <td>https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...</td>\n",
       "      <td>https://weibo.com/6606022745/IwQhN1EAC?refer_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>干杯老铁</td>\n",
       "      <td>\\n                    网友求助: “因为男友我感染了hpv，尿道炎，宫...</td>\n",
       "      <td>父母怎么办</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n                        今天06:02\\n           ...</td>\n",
       "      <td>即刻笔记</td>\n",
       "      <td>https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...</td>\n",
       "      <td>https://weibo.com/3394404550/IwQbUFIBq?refer_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     用户名                                                 博文    关键词  \\\n",
       "0  1杯冰牛奶  \\n                    一直在各个公开场合严肃说了要理智告诉粉丝应该怎样...  父母怎么办   \n",
       "1   干杯老铁  \\n                    网友求助: “因为男友我感染了hpv，尿道炎，宫...  父母怎么办   \n",
       "\n",
       "   是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症） 标签1（担忧对象） 标签2（担忧什么） 标签3（什么保险）  \\\n",
       "0                                           -1       NaN       NaN       NaN   \n",
       "1                                            0       NaN       NaN       NaN   \n",
       "\n",
       "  标签4（症状） 转发数 评论数 点赞数                                               发文时间  \\\n",
       "0     NaN              \\n                        今天06:16\\n           ...   \n",
       "1     NaN              \\n                        今天06:02\\n           ...   \n",
       "\n",
       "          来自                                               页面网址  \\\n",
       "0  iPhone 11  https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...   \n",
       "1       即刻笔记  https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...   \n",
       "\n",
       "                                                博文链接  \n",
       "0  https://weibo.com/6606022745/IwQhN1EAC?refer_f...  \n",
       "1  https://weibo.com/3394404550/IwQbUFIBq?refer_f...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"D:/Users/PYTHON/Precision-Marketing\")\n",
    "df = pd.DataFrame()\n",
    "num = 0\n",
    "for i in range(10):\n",
    "    df_temp = pd.read_excel(\"关键词标签.xlsx\",sheet_name = i)\n",
    "    df = df.append(df_temp)\n",
    "    num += 1\n",
    "print(\"一共读取了{}个sheet\".format(num))\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[pd.notna(df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]),]\n",
    "df_worry = df[df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == 1]\n",
    "df_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = df[df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == -1]\n",
    "df_non_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = process.random_pick(df_non_worry,min(len(df_worry),len(df_non_worry)))\n",
    "df_worry = df_worry[[\"博文\",\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "df_non_worry = df_non_worry[[\"博文\",\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "df_worry = df_worry.dropna()\n",
    "df_non_worry = df_non_worry.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>博文</th>\n",
       "      <th>是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>好想开学啊，网上批作业弄得我都颈椎难受😖再不开学，我害怕我得到暑假回苏格兰计划又要泡汤了 ​</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>\\n                    #钟南山称疫情拐点还要几天#国家安定有多重要？你...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>\\n                    今天拿点东西给我爸临走的时候塞了这三瓶消毒液给我...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>\\n                    我不喜欢说谎，但因为怕父母担心，怕同事亲友闲言碎...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>\\n                    老一辈的生育观点，着眼于“能不能养活”，就是小孩...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    博文  \\\n",
       "539     好想开学啊，网上批作业弄得我都颈椎难受😖再不开学，我害怕我得到暑假回苏格兰计划又要泡汤了 ​   \n",
       "538  \\n                    #钟南山称疫情拐点还要几天#国家安定有多重要？你...   \n",
       "282  \\n                    今天拿点东西给我爸临走的时候塞了这三瓶消毒液给我...   \n",
       "619  \\n                    我不喜欢说谎，但因为怕父母担心，怕同事亲友闲言碎...   \n",
       "627  \\n                    老一辈的生育观点，着眼于“能不能养活”，就是小孩...   \n",
       "\n",
       "     是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）  \n",
       "539                                           -1  \n",
       "538                                           -1  \n",
       "282                                            1  \n",
       "619                                           -1  \n",
       "627                                           -1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_use = pd.concat([df_worry,df_non_worry])\n",
    "df_use.reset_index(drop = True,inplace = True)\n",
    "df_use = df_use.reindex(np.random.permutation(df_use.index))\n",
    "df_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "x_train = process.word_cut(df_use[\"博文\"])\n",
    "x_train = process.frequency(x_train,5)\n",
    "token_index = process.dictionary(x_train)\n",
    "token_length = process.count(x_train)\n",
    "token_length = {key:token_length[key] for key in sorted(token_length,key = lambda x: token_length[x],reverse = True)[:round((2/5)*len(token_index))]}\n",
    "x_train = process.recoding(x_train,process.word_index)\n",
    "x_train = process.delete(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(658, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras import preprocessing\n",
    "\n",
    "max_len = 50\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen = max_len)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_use[[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "y_train[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] = y_train[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"].apply(lambda v: 0 if v == -1 else 1)\n",
    "y_in = len(y_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(y_in)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大的序号是： 880\n"
     ]
    }
   ],
   "source": [
    "def get_values(token_index):\n",
    "    values = []\n",
    "    for key in token_index:\n",
    "        values.append(token_index[key])\n",
    "    return(values)\n",
    "values = get_values(token_index)\n",
    "values[:5]\n",
    "print(\"最大的序号是：\",max(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           3000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,051,401\n",
      "Trainable params: 3,051,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding,LSTM,Bidirectional,Dropout\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,300,input_length = max_len,mask_zero = True)) # 遇到0，就不会反向传播更新权重\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(20),merge_mode = 'concat'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size =128, # batch_size越大越好，但是太大会影响计算效率\n",
    "                    validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"Bi-LSTM(加入预训练的词向量库).h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
