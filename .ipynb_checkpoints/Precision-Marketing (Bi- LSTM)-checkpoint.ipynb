{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class market():\n",
    "    def word_cut(self,documents):\n",
    "        stopwords = self.stopwords\n",
    "        import jieba\n",
    "        texts = []\n",
    "        for line in documents:\n",
    "            words = ' '.join(jieba.cut(line)).split(' ') # ç”¨ç©ºæ ¼å»è¿æ¥ï¼Œè¿æ¥åé©¬ä¸Šåˆæ‹†åˆ†\n",
    "            text = []\n",
    "            for word in words:\n",
    "                if (word not in stopwords) & (word != '')& (word != '\\u3000')& (word != '\\n')&(word != '\\u200b'):\n",
    "                    text.append(word)\n",
    "            texts.append(text)\n",
    "        self.docLength = len(documents)\n",
    "        return(texts)\n",
    "    def get_docLength(self):\n",
    "        return(self.docLength)\n",
    "    def frequency(self,texts,freq):\n",
    "        from collections import defaultdict\n",
    "        frequency = defaultdict(int) # valueä¸ºint\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                frequency[word] += 1\n",
    "        texts = [[word for word in text if frequency[word] > freq] for text in texts]\n",
    "        return(texts)\n",
    "    def regroup(self,texts):\n",
    "        new_texts = []\n",
    "        for i,sentence in enumerate(texts):\n",
    "            new_texts.append(\" \".join(sentence))\n",
    "        return(new_texts)\n",
    "    def add_stopwords(self,path):\n",
    "        stopwords = set()\n",
    "        with open(path,'r',encoding = 'cp936') as file:\n",
    "            for line in file:\n",
    "                stopwords.add(line.strip())\n",
    "        self.stopwords = stopwords\n",
    "        print(\"Load %s stopwords\" %len(stopwords))\n",
    "    def dictionary(self,docs):\n",
    "        token_index ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_index:\n",
    "                    token_index[word] = len(token_index) + 1\n",
    "        return(token_index)\n",
    "    def count(self,docs):\n",
    "        token_length ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_length:\n",
    "                    token_length[word] = 1\n",
    "                else:\n",
    "                    token_length[word] += 1\n",
    "        return(token_length)\n",
    "    def recoding(self,docs,token_index):\n",
    "        for i,sample in enumerate(docs):\n",
    "            for j,word in enumerate(sample):\n",
    "                if word not in token_index:\n",
    "                    sample[j] = -1\n",
    "                else:\n",
    "                    sample[j] = token_index[word]\n",
    "            docs[i] = sample\n",
    "        return(docs)\n",
    "    def delete(self,docs):\n",
    "        for index in range(len(docs)):\n",
    "            for i in range(len(docs[index])-1,-1,-1):\n",
    "                if docs[index][i] == -1:\n",
    "                    docs[index].pop(i)\n",
    "        return docs\n",
    "    def random_pick(self,df,n):\n",
    "        import random\n",
    "        import numpy as np\n",
    "        rand = np.arange(0,(len(df)-1),1)\n",
    "        random.shuffle(rand)\n",
    "        rand = list(rand[:n])\n",
    "        df = df.loc[rand,]\n",
    "        return(df)\n",
    "    def read_vectors(self,path, topn):  # read top n word vectors, i.e. top is 10000\n",
    "        lines_num, dim = 0, 0\n",
    "        vectors = {}\n",
    "        iw = []\n",
    "        wi = {}\n",
    "        with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "            first_line = True\n",
    "            for line in f:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    dim = int(line.rstrip().split()[1]) # åˆ é™¤å‘é‡æœ«å°¾çš„ç©ºæ ¼ï¼Œç„¶åä»¥ç©ºæ ¼æ‹†åˆ†è·å¾—å‘é‡\n",
    "                    continue\n",
    "                lines_num += 1\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])# å½“æ•°æ®æºæ˜¯ndarrayæ—¶ï¼Œasarrayä¸ä¼šå ç”¨æ–°çš„å†…å­˜ï¼›å½“æ•°æ®æºä¸æ˜¯ndarray,asarrayä¸arrayä¸€æ ·\n",
    "                iw.append(tokens[0]) # iwå‚¨å­˜äº†æ‰€æœ‰çš„tokons[0]ï¼Œæ„æ€æ˜¯index_word\n",
    "                if topn != 0 and lines_num >= topn:\n",
    "                    break\n",
    "        for i, w in enumerate(iw):\n",
    "            wi[w] = i # wiæ˜¯iwçš„åè½¬ï¼Œæ„æ€æ˜¯word_index,ç”¨wæ¥å‚¨å­˜å­—ç¬¦ï¼Œç”¨ä¸€ä¸ªintegerå»ç»™å­—ç¬¦ç¼–ç \n",
    "        self.dim = dim\n",
    "        self.max_words = topn\n",
    "        self.word_index = wi\n",
    "        self.index_word = iw\n",
    "        self.vectors = vectors\n",
    "        print(\"Load %s word vectors.\" % len(vectors))\n",
    "    def embedding_matrix(self):\n",
    "        embedding_matrix = np.zeros((self.max_words,self.dim))\n",
    "        for word,i in self.word_index.items():\n",
    "            if i < self.max_words:\n",
    "                embedding_vector = self.vectors.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    def navie_knn(self,dataSet, query, k):  \n",
    "        # è®¡ç®—å‡ºæŸä¸€æ ·æœ¬ä¸æ‰€æœ‰æ ·æœ¬çš„è·ç¦»ï¼Œé€‰æ‹©æœ€å¤§(åº”è¯¥ä¿®æ”¹ä¸ºæœ€å°ï¼Ÿ)çš„kä¸ªæ ·æœ¬ä½œä¸ºç”¨äºknn\n",
    "        numSamples = dataSet.shape[0] # return row(sample) number of dataset\n",
    "\n",
    "        ## step 1: calculate Euclidean distance  \n",
    "        diff = np.tile(query, (numSamples, 1)) - dataSet #tile: æŠŠqueryè¿™ä¸ªå‘é‡çºµå‘å¤åˆ¶ï¼Œä½¿å¾—ç»“æœä¸datasetå…·æœ‰åŒæ ·çš„è¡Œæ•°\n",
    "        squaredDiff = diff ** 2  \n",
    "        squaredDist = np.sum(squaredDiff, axis = 1) # sum is performed by row  \n",
    "\n",
    "        ## step 2: sort the distance  \n",
    "        sortedDistIndices = np.argsort(squaredDist)   # numpy.argsort è¿”å›çš„æ˜¯æ•°ç»„å€¼ä»å°åˆ°å¤§çš„ç´¢å¼•å€¼ï¼ˆæ³¨æ„æ˜¯ç´¢å¼•å€¼ï¼Œä¸æ˜¯ç»å¯¹å€¼ï¼‰\n",
    "        if k > len(sortedDistIndices):  \n",
    "            k = len(sortedDistIndices)  \n",
    "\n",
    "        return sortedDistIndices[0:k]\n",
    "    # build a big graph (normalized weight matrix)  \n",
    "    def buildGraph(self,MatX, kernel_type, rbf_sigma = None, knn_num_neighbors = None):  \n",
    "        num_samples = MatX.shape[0]  # return row(sample) number of MatX\n",
    "        affinity_matrix = np.zeros((num_samples, num_samples), np.float32)  \n",
    "        if kernel_type == 'rbf':  \n",
    "            if rbf_sigma == None:  \n",
    "                raise ValueError('You should input a sigma of rbf kernel!')  \n",
    "            for i in range(num_samples):  \n",
    "                row_sum = 0.0  \n",
    "                for j in range(num_samples):  \n",
    "                    diff = MatX[i, :] - MatX[j, :]  \n",
    "                    affinity_matrix[i][j] = np.exp(sum(diff**2) / (-2.0 * rbf_sigma**2))  \n",
    "                    row_sum += affinity_matrix[i][j]  \n",
    "                affinity_matrix[i][:] /= row_sum  \n",
    "        elif kernel_type == 'knn':  \n",
    "            if knn_num_neighbors == None:  \n",
    "                raise ValueError('You should input a k of knn kernel!')  \n",
    "            for i in range(num_samples):  \n",
    "                k_neighbors = self.navie_knn(MatX, MatX[i, :], knn_num_neighbors)  \n",
    "                affinity_matrix[i][k_neighbors] = 1.0 / knn_num_neighbors  # å°†èŠ‚ç‚¹iä¸é™„è¿‘çš„kä¸ªèŠ‚ç‚¹è¿æ¥èµ·æ¥ï¼Œæ¯ä¸ªè¾¹çš„æƒé‡æ˜¯1/knn_num_neighbors\n",
    "        else:  \n",
    "            raise NameError('Not support kernel type! You can use knn or rbf!')  \n",
    "\n",
    "        return affinity_matrix  \n",
    "    # label propagation  \n",
    "    def labelPropagation(self,Mat_Label, Mat_Unlabel, labels, kernel_type = 'rbf', rbf_sigma = 0.20, \\\n",
    "                        knn_num_neighbors = 10, max_iter = 500, tol = 1e-3):  \n",
    "        # initialize  \n",
    "        num_label_samples = Mat_Label.shape[0]  #å·²ç»æ ‡è®°çš„sample number\n",
    "        num_unlabel_samples = Mat_Unlabel.shape[0]  #æœªæ ‡è®°çš„sample number\n",
    "        num_samples = num_label_samples + num_unlabel_samples\n",
    "        labels_list = np.unique(labels)  #æœ‰å“ªäº›label\n",
    "        num_classes = len(labels_list)  #labelçš„ç§ç±»æ•°\n",
    "\n",
    "        MatX = np.vstack((Mat_Label, Mat_Unlabel))\n",
    "        clamp_data_label = np.zeros((num_label_samples, num_classes), np.float32)  \n",
    "        for i in range(num_label_samples):  \n",
    "            clamp_data_label[i][labels[i]] = 1.0   #æ ‡è®°å‡ºæ¯ä¸€ä¸ªlabelled sampleçš„å…·ä½“labelæ˜¯ä»€ä¹ˆ\n",
    "\n",
    "        label_function = np.zeros((num_samples, num_classes), np.float32)  \n",
    "        label_function[0 : num_label_samples] = clamp_data_label  \n",
    "        label_function[num_label_samples : num_samples] = -1  \n",
    "\n",
    "        # graph construction  \n",
    "        affinity_matrix = self.buildGraph(MatX, kernel_type, rbf_sigma, knn_num_neighbors)  \n",
    "\n",
    "        # start to propagation  \n",
    "        iter = 0; pre_label_function = np.zeros((num_samples, num_classes), np.float32)  \n",
    "        changed = np.abs(pre_label_function - label_function).sum()  \n",
    "        while iter < max_iter and changed > tol:  \n",
    "            if iter % 1 == 0:  \n",
    "                print (\"---> Iteration %d/%d, changed: %f\" % (iter, max_iter, changed))\n",
    "            pre_label_function = label_function  \n",
    "            iter += 1  \n",
    "\n",
    "            # propagation  \n",
    "            label_function = np.dot(affinity_matrix, label_function)  \n",
    "\n",
    "            # clamp  \n",
    "            label_function[0 : num_label_samples] = clamp_data_label  \n",
    "\n",
    "            # check converge  \n",
    "            changed = np.abs(pre_label_function - label_function).sum()  \n",
    "\n",
    "        # get terminate label of unlabeled data  \n",
    "        unlabel_data_labels = np.zeros(num_unlabel_samples)  \n",
    "        for i in range(num_unlabel_samples):  \n",
    "            unlabel_data_labels[i] = np.argmax(label_function[i+num_label_samples]) #å–å‡ºå‚æ•°ä¸­å…ƒç´ æœ€å¤§å€¼æ‰€å¯¹åº”çš„ç´¢å¼• \n",
    "\n",
    "        return unlabel_data_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2316 stopwords\n",
      "Load 10000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "process = market()\n",
    "process.add_stopwords(\"D:/Users/PYTHON/Precision-Marketing/stopwords.txt\")\n",
    "process.read_vectors(\"D:/NLP/sgns.target.word-word.dynwin5.thr10.neg5.dim300.txt\",10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = process.embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸€å…±è¯»å–äº†10ä¸ªsheet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ç”¨æˆ·å</th>\n",
       "      <th>åšæ–‡</th>\n",
       "      <th>å…³é”®è¯</th>\n",
       "      <th>æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰</th>\n",
       "      <th>æ ‡ç­¾1ï¼ˆæ‹…å¿§å¯¹è±¡ï¼‰</th>\n",
       "      <th>æ ‡ç­¾2ï¼ˆæ‹…å¿§ä»€ä¹ˆï¼‰</th>\n",
       "      <th>æ ‡ç­¾3ï¼ˆä»€ä¹ˆä¿é™©ï¼‰</th>\n",
       "      <th>æ ‡ç­¾4ï¼ˆç—‡çŠ¶ï¼‰</th>\n",
       "      <th>è½¬å‘æ•°</th>\n",
       "      <th>è¯„è®ºæ•°</th>\n",
       "      <th>ç‚¹èµæ•°</th>\n",
       "      <th>å‘æ–‡æ—¶é—´</th>\n",
       "      <th>æ¥è‡ª</th>\n",
       "      <th>é¡µé¢ç½‘å€</th>\n",
       "      <th>åšæ–‡é“¾æ¥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1æ¯å†°ç‰›å¥¶</td>\n",
       "      <td>\\n                    ä¸€ç›´åœ¨å„ä¸ªå…¬å¼€åœºåˆä¸¥è‚ƒè¯´äº†è¦ç†æ™ºå‘Šè¯‰ç²‰ä¸åº”è¯¥æ€æ ·...</td>\n",
       "      <td>çˆ¶æ¯æ€ä¹ˆåŠ</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n                        ä»Šå¤©06:16\\n           ...</td>\n",
       "      <td>iPhone 11</td>\n",
       "      <td>https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...</td>\n",
       "      <td>https://weibo.com/6606022745/IwQhN1EAC?refer_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>å¹²æ¯è€é“</td>\n",
       "      <td>\\n                    ç½‘å‹æ±‚åŠ©: â€œå› ä¸ºç”·å‹æˆ‘æ„ŸæŸ“äº†hpvï¼Œå°¿é“ç‚ï¼Œå®«...</td>\n",
       "      <td>çˆ¶æ¯æ€ä¹ˆåŠ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n                        ä»Šå¤©06:02\\n           ...</td>\n",
       "      <td>å³åˆ»ç¬”è®°</td>\n",
       "      <td>https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...</td>\n",
       "      <td>https://weibo.com/3394404550/IwQbUFIBq?refer_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ç”¨æˆ·å                                                 åšæ–‡    å…³é”®è¯  \\\n",
       "0  1æ¯å†°ç‰›å¥¶  \\n                    ä¸€ç›´åœ¨å„ä¸ªå…¬å¼€åœºåˆä¸¥è‚ƒè¯´äº†è¦ç†æ™ºå‘Šè¯‰ç²‰ä¸åº”è¯¥æ€æ ·...  çˆ¶æ¯æ€ä¹ˆåŠ   \n",
       "1   å¹²æ¯è€é“  \\n                    ç½‘å‹æ±‚åŠ©: â€œå› ä¸ºç”·å‹æˆ‘æ„ŸæŸ“äº†hpvï¼Œå°¿é“ç‚ï¼Œå®«...  çˆ¶æ¯æ€ä¹ˆåŠ   \n",
       "\n",
       "   æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰ æ ‡ç­¾1ï¼ˆæ‹…å¿§å¯¹è±¡ï¼‰ æ ‡ç­¾2ï¼ˆæ‹…å¿§ä»€ä¹ˆï¼‰ æ ‡ç­¾3ï¼ˆä»€ä¹ˆä¿é™©ï¼‰  \\\n",
       "0                                           -1       NaN       NaN       NaN   \n",
       "1                                            0       NaN       NaN       NaN   \n",
       "\n",
       "  æ ‡ç­¾4ï¼ˆç—‡çŠ¶ï¼‰ è½¬å‘æ•° è¯„è®ºæ•° ç‚¹èµæ•°                                               å‘æ–‡æ—¶é—´  \\\n",
       "0     NaN              \\n                        ä»Šå¤©06:16\\n           ...   \n",
       "1     NaN              \\n                        ä»Šå¤©06:02\\n           ...   \n",
       "\n",
       "          æ¥è‡ª                                               é¡µé¢ç½‘å€  \\\n",
       "0  iPhone 11  https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...   \n",
       "1       å³åˆ»ç¬”è®°  https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...   \n",
       "\n",
       "                                                åšæ–‡é“¾æ¥  \n",
       "0  https://weibo.com/6606022745/IwQhN1EAC?refer_f...  \n",
       "1  https://weibo.com/3394404550/IwQbUFIBq?refer_f...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"D:/Users/PYTHON/Precision-Marketing\")\n",
    "df = pd.DataFrame()\n",
    "num = 0\n",
    "for i in range(10):\n",
    "    df_temp = pd.read_excel(\"å…³é”®è¯æ ‡ç­¾.xlsx\",sheet_name = i)\n",
    "    df = df.append(df_temp)\n",
    "    num += 1\n",
    "print(\"ä¸€å…±è¯»å–äº†{}ä¸ªsheet\".format(num))\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[pd.notna(df[\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"]),]\n",
    "df_worry = df[df[\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"] == 1]\n",
    "df_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = df[df[\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"] == -1]\n",
    "df_non_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = process.random_pick(df_non_worry,min(len(df_worry),len(df_non_worry)))\n",
    "df_worry = df_worry[[\"åšæ–‡\",\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"]]\n",
    "df_non_worry = df_non_worry[[\"åšæ–‡\",\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"]]\n",
    "df_worry = df_worry.dropna()\n",
    "df_non_worry = df_non_worry.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>åšæ–‡</th>\n",
       "      <th>æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>å¥½æƒ³å¼€å­¦å•Šï¼Œç½‘ä¸Šæ‰¹ä½œä¸šå¼„å¾—æˆ‘éƒ½é¢ˆæ¤éš¾å—ğŸ˜–å†ä¸å¼€å­¦ï¼Œæˆ‘å®³æ€•æˆ‘å¾—åˆ°æš‘å‡å›è‹æ ¼å…°è®¡åˆ’åˆè¦æ³¡æ±¤äº† â€‹</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>\\n                    #é’Ÿå—å±±ç§°ç–«æƒ…æ‹ç‚¹è¿˜è¦å‡ å¤©#å›½å®¶å®‰å®šæœ‰å¤šé‡è¦ï¼Ÿä½ ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>\\n                    ä»Šå¤©æ‹¿ç‚¹ä¸œè¥¿ç»™æˆ‘çˆ¸ä¸´èµ°çš„æ—¶å€™å¡äº†è¿™ä¸‰ç“¶æ¶ˆæ¯’æ¶²ç»™æˆ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>\\n                    æˆ‘ä¸å–œæ¬¢è¯´è°ï¼Œä½†å› ä¸ºæ€•çˆ¶æ¯æ‹…å¿ƒï¼Œæ€•åŒäº‹äº²å‹é—²è¨€ç¢...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>\\n                    è€ä¸€è¾ˆçš„ç”Ÿè‚²è§‚ç‚¹ï¼Œç€çœ¼äºâ€œèƒ½ä¸èƒ½å…»æ´»â€ï¼Œå°±æ˜¯å°å­©...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    åšæ–‡  \\\n",
       "539     å¥½æƒ³å¼€å­¦å•Šï¼Œç½‘ä¸Šæ‰¹ä½œä¸šå¼„å¾—æˆ‘éƒ½é¢ˆæ¤éš¾å—ğŸ˜–å†ä¸å¼€å­¦ï¼Œæˆ‘å®³æ€•æˆ‘å¾—åˆ°æš‘å‡å›è‹æ ¼å…°è®¡åˆ’åˆè¦æ³¡æ±¤äº† â€‹   \n",
       "538  \\n                    #é’Ÿå—å±±ç§°ç–«æƒ…æ‹ç‚¹è¿˜è¦å‡ å¤©#å›½å®¶å®‰å®šæœ‰å¤šé‡è¦ï¼Ÿä½ ...   \n",
       "282  \\n                    ä»Šå¤©æ‹¿ç‚¹ä¸œè¥¿ç»™æˆ‘çˆ¸ä¸´èµ°çš„æ—¶å€™å¡äº†è¿™ä¸‰ç“¶æ¶ˆæ¯’æ¶²ç»™æˆ‘...   \n",
       "619  \\n                    æˆ‘ä¸å–œæ¬¢è¯´è°ï¼Œä½†å› ä¸ºæ€•çˆ¶æ¯æ‹…å¿ƒï¼Œæ€•åŒäº‹äº²å‹é—²è¨€ç¢...   \n",
       "627  \\n                    è€ä¸€è¾ˆçš„ç”Ÿè‚²è§‚ç‚¹ï¼Œç€çœ¼äºâ€œèƒ½ä¸èƒ½å…»æ´»â€ï¼Œå°±æ˜¯å°å­©...   \n",
       "\n",
       "     æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰  \n",
       "539                                           -1  \n",
       "538                                           -1  \n",
       "282                                            1  \n",
       "619                                           -1  \n",
       "627                                           -1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_use = pd.concat([df_worry,df_non_worry])\n",
    "df_use.reset_index(drop = True,inplace = True)\n",
    "df_use = df_use.reindex(np.random.permutation(df_use.index))\n",
    "df_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "x_train = process.word_cut(df_use[\"åšæ–‡\"])\n",
    "x_train = process.frequency(x_train,5)\n",
    "token_index = process.dictionary(x_train)\n",
    "token_length = process.count(x_train)\n",
    "token_length = {key:token_length[key] for key in sorted(token_length,key = lambda x: token_length[x],reverse = True)[:round((2/5)*len(token_index))]}\n",
    "x_train = process.recoding(x_train,process.word_index)\n",
    "x_train = process.delete(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(658, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras import preprocessing\n",
    "\n",
    "max_len = 50\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen = max_len)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df_use[[\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"]]\n",
    "y_train[\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"] = y_train[\"æ˜¯å¦æ‹…å¿§ï¼ˆ1=æ‹…å¿§ï¼Œ-1=å®Œå…¨ä¸æ‹…å¿§ï¼Œ0=ä¸­æ€§ï¼Œæœ‰äº›æ‹…å¿§ä½†ä¸ç”¨ä¹°ä¿é™©,2=ç–‘ä¼¼æŠ‘éƒç—‡ï¼‰\"].apply(lambda v: 0 if v == -1 else 1)\n",
    "y_in = len(y_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(y_in)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€å¤§çš„åºå·æ˜¯ï¼š 880\n"
     ]
    }
   ],
   "source": [
    "def get_values(token_index):\n",
    "    values = []\n",
    "    for key in token_index:\n",
    "        values.append(token_index[key])\n",
    "    return(values)\n",
    "values = get_values(token_index)\n",
    "values[:5]\n",
    "print(\"æœ€å¤§çš„åºå·æ˜¯ï¼š\",max(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           3000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,051,401\n",
      "Trainable params: 3,051,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding,LSTM,Bidirectional,Dropout\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,300,input_length = max_len,mask_zero = True)) # é‡åˆ°0ï¼Œå°±ä¸ä¼šåå‘ä¼ æ’­æ›´æ–°æƒé‡\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(20),merge_mode = 'concat'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size =128, # batch_sizeè¶Šå¤§è¶Šå¥½ï¼Œä½†æ˜¯å¤ªå¤§ä¼šå½±å“è®¡ç®—æ•ˆç‡\n",
    "                    validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"Bi-LSTM(åŠ å…¥é¢„è®­ç»ƒçš„è¯å‘é‡åº“).h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
