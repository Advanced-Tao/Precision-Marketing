{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import jieba\n",
    "import random\n",
    "\n",
    "'''\n",
    "********************************************************************************************************\n",
    "Precision-Marketing (Bi- LSTM).py\n",
    "class:market\n",
    "\n",
    "author: Andy\n",
    "date:2020/3/18\n",
    "\n",
    "market类的主要目的是分析新浪微博的博文数据（爬取的标签如“会不会得病”、“身体不舒服”等），分析博文中的担忧情绪。\n",
    "因为有担忧情绪的人更有可能购买健康保险，所以这类人群是健康保险产品的潜在客户。\n",
    "\n",
    "该类主要功能有以下三个：\n",
    "1. 导入停用词库和词向量库(函数名：add_stopwords，read_vectors)\n",
    "2. 构建embedding层（函数名：embedding_matrix） \n",
    "3. 将非结构化的文本数据处理成结构化的形式(函数名： word_cut,frequency，recoding，delete)\n",
    "\n",
    "********************************************************************************************************\n",
    "'''\n",
    "class market():\n",
    "\n",
    "\n",
    "    '''\n",
    "name: read_vectors\n",
    "function（函数的功能）: 载入预训练的词向量库中的前topn个词向量\n",
    "input:词向量库的存放路径(path), 导入的词语数目(topn)\n",
    "output: 词向量的维度数（self.dim，如每个词语提取了300个特征，则dim=300）\n",
    "提取的词语数量（self.max_words，如果提取10000个词向量，则self.max_words = 10000）\n",
    "词语字典(self.word_index, 返回的结果是一个字典。每个词语对应一个整数编码\n",
    "比如{\"的\"：1，\"非常\":2，\"生病\":3})\n",
    "整数字典（self.index_word，每一个整数对应一个词语编码，该词典意义不大，已弃用。）\n",
    "词向量(self.vectors，返回的结果是一个字典。每一个词语对应着dim个特征，如果dim=300，则每一个词都对应着一个1×300的向量，\n",
    "比如{\"的\"：array[0.525421355,0.15235234,......],\"非常\":array[0.3535353111,0.3543636321,......]})\n",
    "'''\n",
    "    def read_vectors(self,path, topn):  \n",
    "        lines_num, dim = 0, 0\n",
    "        vectors = {}\n",
    "        iw = []\n",
    "        wi = {}\n",
    "        with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "            first_line = True\n",
    "            for line in f:\n",
    "                if first_line:\n",
    "                    first_line = False\n",
    "                    dim = int(line.rstrip().split()[1]) \n",
    "                    continue\n",
    "                lines_num += 1\n",
    "                tokens = line.rstrip().split(' ')\n",
    "                vectors[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
    "                iw.append(tokens[0]) # iw储存了所有的tokons[0]，意思是index_word\n",
    "                if topn != 0 and lines_num >= topn:\n",
    "                    break\n",
    "        for i, w in enumerate(iw):\n",
    "            wi[w] = i # wi是iw的反转，意思是word_index,用w来储存字符，用一个integer去给字符编码\n",
    "        self.dim = dim\n",
    "        self.max_words = topn\n",
    "        self.word_index = wi\n",
    "        self.index_word = iw\n",
    "        self.vectors = vectors\n",
    "        print(\"Load %s word vectors.\" % len(vectors))\n",
    "\n",
    "        '''\n",
    "name: add_stopwords\n",
    "function: 导入停用词库\n",
    "input:停用词库的存放路径(path)\n",
    "output: 停用词集合（self.stopwords，返回的结果是一个集合(set)，集合中储存了几百个停用词。\n",
    "采用了改进后四川大学提供的停用词库，该词库中加入了保险中的特有名词，如“健康保险”）\n",
    "'''\n",
    "    def add_stopwords(self,path):\n",
    "        stopwords = set()\n",
    "        with open(path,'r',encoding = 'cp936') as file:\n",
    "            for line in file:\n",
    "                stopwords.add(line.strip())\n",
    "        self.stopwords = stopwords\n",
    "        print(\"Load %s stopwords\" %len(stopwords))    \n",
    "\n",
    "        '''\n",
    "name: add_stopwords\n",
    "function: 构建Embedding矩阵，该矩阵维度数目为 词语数量 × 特征数量（比如10000×300），在神经网络中通过该层，\n",
    "可以将每个词语编码成300个维度的密集向量\n",
    "input: 无\n",
    "output: Embedding矩阵（embedding_matrix）\n",
    "'''\n",
    "    def embedding_matrix(self):\n",
    "        embedding_matrix = np.zeros((self.max_words,self.dim))\n",
    "        for word,i in self.word_index.items():\n",
    "            if i < self.max_words:\n",
    "                embedding_vector = self.vectors.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "        return embedding_matrix\n",
    "\n",
    "    '''\n",
    "name: word_cut\n",
    "function: 将博文分割成几个词语\n",
    "input: 储存博文的文档（documents。该文档是一个不定长的list，长度为博文数量，但宽度未知（因为博文不定长），\n",
    "比如[[\"今天胸口疼痛不舒服，自己会不会得病啊？\"][\"上班压力好大，长期下来积劳成疾怎么办\"]......]\n",
    "output: 按照一定规则，将博文切割成词语后的文档（texts，该文档仍然是以个不定长的list，不同词语之间以逗号分割，\n",
    "比如[[\"今天,胸口,疼痛,不舒服，自己,会不会,得病,啊？\"][\"上班,压力,好大，长期下来,积劳成疾,怎么办\"]......]）\n",
    "'''\n",
    "    def word_cut(self,documents):\n",
    "        stopwords = self.stopwords\n",
    "        texts = []\n",
    "        for line in documents:\n",
    "            words = ' '.join(jieba.cut(line)).split(' ') # 用空格去连接，连接后马上又拆分\n",
    "            text = []\n",
    "            for word in words:\n",
    "                if (word not in stopwords) & (word != '')& (word != '\\u3000')& (word != '\\n')&(word != '\\u200b'):\n",
    "                    text.append(word)\n",
    "            texts.append(text)\n",
    "        self.docLength = len(documents)\n",
    "        return(texts)\n",
    "\n",
    "    '''\n",
    "name: frequency\n",
    "function: 按照词语的出现频次过滤掉某些低频次\n",
    "input: 切割成词语后的文档（texts）,允许的最低出现频率（freq，比如freq=5，意味着删掉在所有词语中出现次数 <= 5的词语）\n",
    "output: 过滤后的文档(texts)\n",
    "'''\n",
    "    def frequency(self,texts,freq):\n",
    "        frequency = defaultdict(int) # value为int\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                frequency[word] += 1\n",
    "        texts = [[word for word in text if frequency[word] > freq] for text in texts]\n",
    "        return(texts)\n",
    "\n",
    "    '''\n",
    "name: recoding\n",
    "function: 将词语编码成整数形式，如果词典（word_index）中没有该词语，则编码为-1\n",
    "input: 过滤后的文档(texts)，词典（word_index）\n",
    "output: 将词语按照整数编码后的文档(texts)\n",
    "'''\n",
    "    def recoding(self,texts,word_index):\n",
    "        for i,sample in enumerate(texts):\n",
    "            for j,word in enumerate(sample):\n",
    "                if word not in word_index:\n",
    "                    sample[j] = -1\n",
    "                else:\n",
    "                    sample[j] = word_index[word]\n",
    "            texts[i] = sample\n",
    "        return(texts)\n",
    "\n",
    "    '''\n",
    "name: delete\n",
    "function: 将文档中编码为-1的记录删去\n",
    "input: 将词语按照整数编码后的文档(docs)\n",
    "output: 删除了所有编码为-1的记录的文档（docs）\n",
    "'''\n",
    "    def delete(self,docs):\n",
    "        for index in range(len(docs)):\n",
    "            for i in range(len(docs[index])-1,-1,-1):\n",
    "                if docs[index][i] == -1:\n",
    "                    docs[index].pop(i)\n",
    "        return docs\n",
    "\n",
    "    '''\n",
    "name: random_pick\n",
    "function: 对不担忧的样本做欠采样，比如不担忧的样本有936个，担忧的样本有300个，则欠采样的结果是从不担忧的936个样本里面随机选取300个\n",
    "input: 需要欠采样的数据框（df），欠采样之后的样本数量（n）\n",
    "output: 欠采样后的数据框(df)\n",
    "'''\n",
    "    def random_pick(self,df,n):\n",
    "        rand = np.arange(0,(len(df)-1),1)\n",
    "        random.shuffle(rand)\n",
    "        rand = list(rand[:n])\n",
    "        df = df.loc[rand,]\n",
    "        return(df)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "因为预训练的词向量库中已预含词典，所以该函数被废弃\n",
    "    def dictionary(self,docs):\n",
    "        token_index ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_index:\n",
    "                    token_index[word] = len(token_index) + 1\n",
    "        return(token_index)\n",
    "'''\n",
    "\n",
    "    '''\n",
    "因为预训练的词向量库中已预含词典，所以该函数被废弃\n",
    "    def count(self,docs):\n",
    "        token_length ={}\n",
    "        for sample in docs:\n",
    "            for word in sample:\n",
    "                if word not in token_length:\n",
    "                    token_length[word] = 1\n",
    "                else:\n",
    "                    token_length[word] += 1\n",
    "        return(token_length)\n",
    "'''   \n",
    "\n",
    "    '''\n",
    "因为暂停了文本聚类项目，所以该函数被废弃\n",
    "    def regroup(self,texts):\n",
    "        new_texts = []\n",
    "        for i,sentence in enumerate(texts):\n",
    "            new_texts.append(\" \".join(sentence))\n",
    "        return(new_texts)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 2316 stopwords\n",
      "Load 10000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 导入停用词库和词向量库\n",
    "# 词向量库由北师大提供(github地址\"https://github.com/Embedding/Chinese-Word-Vectors\")，\n",
    "# 停用词库由四川大学提供(github地址：\"https://github.com/fighting41love/funNLP/tree/master/data/%E5%81%9C%E7%94%A8%E8%AF%8D\")\n",
    "\n",
    "process = market()\n",
    "process.add_stopwords(\"D:/Users/PYTHON/Precision-Marketing/stopwords.txt\")\n",
    "process.read_vectors(\"D:/NLP/sgns.target.word-word.dynwin5.thr10.neg5.dim300.txt\",10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建词矩阵（由若干个词向量堆叠而成，如果导入了10000个词，每个词有300个特征，则矩阵维度为10000×300），\n",
    "# 该步骤可以为下文嵌入keras的embedding层做铺垫\n",
    "embedding_matrix = process.embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共读取了10个sheet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>用户名</th>\n",
       "      <th>博文</th>\n",
       "      <th>关键词</th>\n",
       "      <th>是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）</th>\n",
       "      <th>标签1（担忧对象）</th>\n",
       "      <th>标签2（担忧什么）</th>\n",
       "      <th>标签3（什么保险）</th>\n",
       "      <th>标签4（症状）</th>\n",
       "      <th>转发数</th>\n",
       "      <th>评论数</th>\n",
       "      <th>点赞数</th>\n",
       "      <th>发文时间</th>\n",
       "      <th>来自</th>\n",
       "      <th>页面网址</th>\n",
       "      <th>博文链接</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1杯冰牛奶</td>\n",
       "      <td>\\n                    一直在各个公开场合严肃说了要理智告诉粉丝应该怎样...</td>\n",
       "      <td>父母怎么办</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n                        今天06:16\\n           ...</td>\n",
       "      <td>iPhone 11</td>\n",
       "      <td>https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...</td>\n",
       "      <td>https://weibo.com/6606022745/IwQhN1EAC?refer_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>干杯老铁</td>\n",
       "      <td>\\n                    网友求助: “因为男友我感染了hpv，尿道炎，宫...</td>\n",
       "      <td>父母怎么办</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\n                        今天06:02\\n           ...</td>\n",
       "      <td>即刻笔记</td>\n",
       "      <td>https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...</td>\n",
       "      <td>https://weibo.com/3394404550/IwQbUFIBq?refer_f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     用户名                                                 博文    关键词  \\\n",
       "0  1杯冰牛奶  \\n                    一直在各个公开场合严肃说了要理智告诉粉丝应该怎样...  父母怎么办   \n",
       "1   干杯老铁  \\n                    网友求助: “因为男友我感染了hpv，尿道炎，宫...  父母怎么办   \n",
       "\n",
       "   是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症） 标签1（担忧对象） 标签2（担忧什么） 标签3（什么保险）  \\\n",
       "0                                           -1       NaN       NaN       NaN   \n",
       "1                                            0       NaN       NaN       NaN   \n",
       "\n",
       "  标签4（症状） 转发数 评论数 点赞数                                               发文时间  \\\n",
       "0     NaN              \\n                        今天06:16\\n           ...   \n",
       "1     NaN              \\n                        今天06:02\\n           ...   \n",
       "\n",
       "          来自                                               页面网址  \\\n",
       "0  iPhone 11  https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...   \n",
       "1       即刻笔记  https://s.weibo.com/weibo/%25E7%2588%25B6%25E6...   \n",
       "\n",
       "                                                博文链接  \n",
       "0  https://weibo.com/6606022745/IwQhN1EAC?refer_f...  \n",
       "1  https://weibo.com/3394404550/IwQbUFIBq?refer_f...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入数据，数据来源于微博，已经人工标注。数据可以从github上下载\n",
    "# （github地址：https://github.com/Advanced-Tao/Precision-Marketing/master/关键词标签.xlsx）\n",
    "os.chdir(\"D:/Users/PYTHON/Precision-Marketing\")\n",
    "df = pd.DataFrame()\n",
    "num = 0\n",
    "for i in range(10):\n",
    "    df_temp = pd.read_excel(\"关键词标签.xlsx\",sheet_name = i)\n",
    "    df = df.append(df_temp)\n",
    "    num += 1\n",
    "print(\"一共读取了{}个sheet\".format(num))\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对无担忧情绪（无买保险欲望）的标签(df_non_worry)做欠采样\n",
    "\n",
    "df = df.loc[pd.notna(df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]),]\n",
    "df_worry = df[df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == 1]\n",
    "df_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = df[df[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] == -1]\n",
    "df_non_worry.reset_index(drop = True,inplace = True)\n",
    "df_non_worry = process.random_pick(df_non_worry,min(len(df_worry),len(df_non_worry)))\n",
    "df_worry = df_worry[[\"博文\",\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "df_non_worry = df_non_worry[[\"博文\",\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "df_worry = df_worry.dropna()\n",
    "df_non_worry = df_non_worry.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>博文</th>\n",
       "      <th>是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>\\n                    #利刃出鞘# 看完之后，感觉是悬疑里面悬念制造还...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>\\n                    取卵后月经总提前！ 之前都是2.3天！ 这两个月...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>\\n                    罗小黑战记 （我太蠢了发错超话了，重发x） c...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>\\n                    突然之间，天旋地转，差点摔了我会不会得病了 2高...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\\n                    儿子手足口，一周跑人民，三点逃班，七点回家，半夜...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    博文  \\\n",
       "389  \\n                    #利刃出鞘# 看完之后，感觉是悬疑里面悬念制造还...   \n",
       "122  \\n                    取卵后月经总提前！ 之前都是2.3天！ 这两个月...   \n",
       "597  \\n                    罗小黑战记 （我太蠢了发错超话了，重发x） c...   \n",
       "251  \\n                    突然之间，天旋地转，差点摔了我会不会得病了 2高...   \n",
       "18   \\n                    儿子手足口，一周跑人民，三点逃班，七点回家，半夜...   \n",
       "\n",
       "     是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）  \n",
       "389                                           -1  \n",
       "122                                            1  \n",
       "597                                           -1  \n",
       "251                                            1  \n",
       "18                                             1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并欠采样后的无担忧标签和有担忧标签，得到类别数目平衡的数据框df_use\n",
    "\n",
    "df_use = pd.concat([df_worry,df_non_worry])\n",
    "df_use.reset_index(drop = True,inplace = True)\n",
    "df_use = df_use.reindex(np.random.permutation(df_use.index))\n",
    "df_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.632 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 自变量预处理\n",
    "\n",
    "x_train = process.word_cut(df_use[\"博文\"])\n",
    "x_train = process.frequency(x_train,5)\n",
    "x_train = process.recoding(x_train,process.word_index)\n",
    "x_train = process.delete(x_train)\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras import preprocessing\n",
    "\n",
    "max_len = 50\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen = max_len) # 将博文50个字符以后的部分舍弃\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因变量预处理\n",
    "\n",
    "y_train = df_use[[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"]]\n",
    "y_train[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"] = y_train[\"是否担忧（1=担忧，-1=完全不担忧，0=中性，有些担忧但不用买保险,2=疑似抑郁症）\"].apply(lambda v: 0 if v == -1 else 1)\n",
    "y_in = len(y_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(y_in)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           3000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 40)                51360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 3,051,401\n",
      "Trainable params: 3,051,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建神经网络模型\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding,LSTM,Bidirectional,Dropout\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,300,input_length = max_len,mask_zero = True)) # 遇到0，就不会反向传播更新权重\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(20),merge_mode = 'concat'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练神经网络模型\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size =128, # batch_size越大越好，但是太大会影响计算效率\n",
    "                    validation_split= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存神经网络\n",
    "\n",
    "# model.save(\"Bi-LSTM(加入预训练的词向量库).h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
